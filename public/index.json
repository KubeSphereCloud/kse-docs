
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  [{"content":" 本节介绍 KubeSphere 企业版的环境要求。\n系统要求 系统 最低要求（每个节点） 生产环境要求（每个节点） Ubuntu 16.04，18.04，20.04，22.04\nCPU：2 核，内存：4 GB，硬盘：40 GB\nCPU：8 核，内存：16 GB，硬盘：200 GB\nDebian Buster，Stretch\nCPU：2 核，内存：4 GB，硬盘：40 GB\nCPU：8 核，内存：16 GB，硬盘：200 GB\nCentOS 7.x，CentOS Stream\nCPU：2 核，内存：4 GB，硬盘：40 GB\nCPU：8 核，内存：16 GB，硬盘：200 GB\nRed Hat Enterprise Linux 7.x，8.x\nCPU：2 核，内存：4 GB，硬盘：40 GB\nCPU：8 核，内存：16 GB，硬盘：200 GB\nSUSE Linux Enterprise Server 15/openSUSE Leap 15.2\nCPU：2 核，内存：4 GB，硬盘：40 GB\nCPU：8 核，内存：16 GB，硬盘：200 GB\n存储要求 集群中必须存在一个可用的默认存储类。\n存储类定义了可供容器使用的一类存储卷。如果您在安装 KubeSphere 企业版时未设置外部持久化存储系统，KubeSphere 企业版将使用集群节点的本地存储系统作为持久化存储系统，并自动创建对应的 local 存储类。如果使用外部持久化存储系统，您需要为 KubeSphere 企业版集群安装存储插件，并创建存储类以定义可供使用的存储卷类型。有关如何安装存储插件，请联系您的存储系统提供商或参阅配置外部持久化存储。\nKubeSphere 企业版的各个组件只需要存储系统提供创建、删除持久卷声明（PVC）的能力，不依赖扩容、克隆、快照等高级能力。\n依赖项要求 依赖项 Kubernetes 版本 ≥ 1.18 Kubernetes 版本 \u0026lt; 1.18 socat\n必须\n可选，但建议安装\nconntrack\n必须\n可选，但建议安装\nebtables\n可选，但建议安装\n可选，但建议安装\nipset\n可选，但建议安装\n可选，但建议安装\n容器运行时要求 支持的容器运行时 版本 Docker\n20.10.0+\ncontainerd\n最新版\nCRI-O（试验版，未经充分测试）\n最新版\niSula（试验版，未经充分测试）\n最新版\n网络要求 请确保 /etc/resolv.conf 中的 DNS 地址可用，否则，可能会导致集群中的 DNS 出现问题。\n如果您的网络配置使用防火墙规则或安全组，请务必确保基础设施组件可以通过特定端口相互通信。建议您关闭防火墙。\n支持的 CNI 插件：Calico 和 Flannel。其他插件也适用（例如 Cilium 和 Kube-OVN 等），但请注意它们未经充分测试。\n端口要求 某些端口需要用于服务之间的通信。如果您的网络配置有防火墙规则，则需要确保基础设施组件可以通过特定端口相互通信。这些端口用作某些进程或服务的通信端点。\n服务 协议 行为 起始端口 结束端口 备注 ssh\nTCP\nallow\n22\nN/A\nN/A\netcd\nTCP\nallow\n2379\n2380\nN/A\napiserver\nTCP\nallow\n6443\nN/A\nN/A\ncalico\nTCP\nallow\n9099\n9100\nN/A\nbgp\nTCP\nallow\n179\nN/A\nN/A\nnodeport\nTCP\nallow\n30000\n32767\nN/A\nmaster\nTCP\nallow\n10250\n10258\nN/A\ndns\nTCP\nallow\n53\nN/A\nN/A\ndns\nUDP\nallow\n53\nN/A\nN/A\nlocal-registry\nTCP\nallow\n5000\nN/A\n离线环境需要\nlocal-apt\nTCP\nallow\n5080\nN/A\n离线环境需要\nrpcbind\nTCP\nallow\n111\nN/A\n使用 NFS 时需要\nipip\nIPENCAP / IPIP\nallow\nN/A\nN/A\nCalico 需要使用 IPIP 协议\nmetrics-server\nTCP\nallow\n8443\nN/A\nN/A\n组件支持矩阵 KubeSphere 企业版 v4.1.3 默认支持 Kubernetes v1.21~1.30。本节详细介绍 KubeSphere 企业版各组件支持的 Kubernetes 版本。为避免使用过程中出现兼容性问题，请使用支持的 Kubernetes 版本。\n组件 组件版本 支持的 Kubernetes 版本 支持的架构 ks-core\n4.1.x\n1.21~1.30\namd64、arm64\nDevOps\n1.1.x\n1.21~1.30\namd64、arm64\nStorage-utils\n1.0.x\n1.21~1.30\namd64、arm64\nVector\n1.0.x\n1.21~1.30\namd64、arm64\nOpenSearch\n2.11.1\n1.21~1.30\namd64、arm64\nMonitoring\n1.1.x\n1.21~1.30\namd64、arm64\nLogging\n1.2.x\n1.21~1.30\namd64、arm64\nAuditing\n1.2.x\n1.21~1.30\namd64、arm64\nEvents\n1.2.x\n1.21~1.30\namd64、arm64\nAlerting\n1.0.x\n1.21~1.30\namd64、arm64\nNotification\n2.6.x\n1.21~1.30\namd64、arm64\nWizTelemetry 平台服务\n1.3.x\n1.21~1.30\namd64、arm64\nWizTelemetry 事件告警\n1.2.x\n1.21~1.30\namd64、arm64\nGrafana for WizTelemetry\n10.4.x\n1.21~1.30\namd64、arm64\nGrafana Loki for WizTelemetry\n1.0.x\n1.21~1.30\namd64、arm64\nGrafana Alloy for WizTelemetry\n1.0.x\n1.21~1.30\namd64、arm64\nMetricServer\n0.7.0\n1.21~1.30\namd64、arm64\nTower\n1.0.x\n1.21~1.30\namd64、arm64\nOpenPitrix\n2.0.x\n1.21~1.30\namd64、arm64\nKubeFed\n1.0.x\n1.21~1.30\namd64、arm64\nGateway\n1.0.x\n1.22~1.30\namd64、arm64\nServiceMesh\n1.0.x\n1.22~1.30\namd64、arm64\nKubeEdge\n1.13.1\n1.21-1.23\namd64、arm64\nRadonDB DMP\n2.1.4\nDMP 管理平台: 1.21~1.28\nMySQL: 1.21~1.28\nPostgreSQL: 1.21~1.28\nRedis Cluster: 1.21~1.28\nRedis Sentinel: 1.21~1.28\nMongoDB: 1.21~1.28\nOpenSearch: 1.21~1.28\nKafka: 1.21~1.28\nRabbitMQ: 1.21~1.28\namd64\nSpringCloud\n1.0.x\n1.21~1.30\namd64、arm64\nGatekeeper\n1.0.x\n1.21~1.30\namd64、arm64\nNetwork\n1.1.x\n1.21~1.30\namd64、arm64\nIngress-utils\n1.0.x\n1.21~1.30\namd64、arm64\noauth2-proxy\n7.6.x\n1.21~1.30\namd64、arm64\nnvidia-gpu-operator\n23.9.x\n1.21~1.30\namd64、arm64\ncert-manager\n1.0.x\n1.21~1.30\namd64、arm64\n","href":"/v4.1.3/03-installation-and-upgrade/01-preparations/01-supported-k8s/","isSection":null,"linkkey":null,"title":"环境要求"},{"content":" 本节介绍如何卸载 KubeSphere 企业版。当前集群中运行的 Kubernetes 将不会被卸载。\n警告 虽然此操作不会卸载 Kubernetes，如果当前集群中运行的业务使用了 KubeSphere 企业版提供的功能，此操作仍然可能导致业务中断。\n此操作不能撤销，请谨慎执行此操作。\n前提条件 为避免数据丢失，请提前备份所有重要数据。\n卸载扩展组件 登录任意集群节点，执行以下命令查看集群中已安装的扩展组件。\nkubectl get installplan 卸载指定扩展组件或所有扩展组件。\n卸载指定扩展组件。\nkubectl delete installplan {InstallPlan Name} 说明 可根据第一步的命令获取扩展组件的 InstallPlan Name。\n如果显示如下信息，表明该扩展组件卸载成功。\ninstallplan.kubesphere.io \u0026#34;{InstallPlan Name}\u0026#34; deleted 卸载全部扩展组件。\nkubectl delete installplan --all 输出信息应如下所示：\ninstallplan.kubesphere.io \u0026#34;devops\u0026#34; deleted installplan.kubesphere.io \u0026#34;dmp\u0026#34; deleted installplan.kubesphere.io \u0026#34;gatekeeper\u0026#34; deleted installplan.kubesphere.io \u0026#34;gateway\u0026#34; deleted installplan.kubesphere.io \u0026#34;kubeedge\u0026#34; deleted installplan.kubesphere.io \u0026#34;kubefed\u0026#34; deleted installplan.kubesphere.io \u0026#34;metrics-server\u0026#34; deleted installplan.kubesphere.io \u0026#34;network\u0026#34; deleted installplan.kubesphere.io \u0026#34;openpitrix\u0026#34; deleted installplan.kubesphere.io \u0026#34;opensearch\u0026#34; deleted installplan.kubesphere.io \u0026#34;springcloud\u0026#34; deleted installplan.kubesphere.io \u0026#34;storage-utils\u0026#34; deleted installplan.kubesphere.io \u0026#34;tower\u0026#34; deleted installplan.kubesphere.io \u0026#34;vector\u0026#34; deleted installplan.kubesphere.io \u0026#34;whizard-alerting\u0026#34; deleted installplan.kubesphere.io \u0026#34;whizard-auditing\u0026#34; deleted installplan.kubesphere.io \u0026#34;whizard-events\u0026#34; deleted installplan.kubesphere.io \u0026#34;whizard-logging\u0026#34; deleted installplan.kubesphere.io \u0026#34;whizard-monitoring\u0026#34; deleted installplan.kubesphere.io \u0026#34;whizard-notification\u0026#34; deleted installplan.kubesphere.io \u0026#34;whizard-telemetry\u0026#34; deleted installplan.kubesphere.io \u0026#34;whizard-telemetry-ruler\u0026#34; deleted 再次执行以下命令，如果显示 No resources found，表明所有扩展组件都已卸载。\nkubectl get installplan 卸载 ks-core 卸载 ks-core 之前，请确保集群中的扩展组件都已被卸载，即执行 kubectl get installplan 命令后显示 No resources found。\n执行以下命令卸载 ks-core。\nhelm del -n kubesphere-system ks-core 执行以下命令，如果返回结果为空（如下所示），表明 KubeSphere 企业版卸载成功。\nroot@xxx:~# helm list -n kubesphere-system NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION ","href":"/v4.1.3/03-installation-and-upgrade/04-uninstall-kubesphere/01-uninstall-kubesphere-only/","isSection":null,"linkkey":null,"title":"仅卸载{ks_product_left}"},{"content":" 本节介绍如何在生产环境中为 KubeSphere 企业版配置云上存储设备。\n您可以在云环境中创建 API 密钥，并设置 KubeSphere 企业版使用 API 密钥通过容器存储接口（CSI）与云环境对接。用户在 KubeSphere 企业版平台上创建卷并将卷挂载到容器组时，KubeSphere 企业版将自动在云环境中创建存储设备供容器组使用。以下以青云QingCloud 为例介绍具体操作。有关其他云环境中的操作，请参阅云环境的用户指南或联系您的云服务提供商。\n前提条件 您需要获取一个青云QingCloud 帐户，并确保帐户余额可以创建所需的存储设备。有关更多信息，请访问青云QingCloud 官网。\n操作步骤 登录青云QingCloud 控制台，点击页面右上角的用户名，然后在下拉列表中选择 API 密钥。\n在 API 密钥页面，点击创建。\n在创建API密钥对话框，设置密钥的名称，然后点击提交。\n在弹出的对话框将私钥文件下载到本地。\n警告 KubeSphere 企业版将使用该私钥与云上的存储设备对接。请妥善保存该私钥文件以避免用户数据泄露。\n在 API 密钥列表中获取 API 密钥的 ID。\n登录用于执行 KubeSphere 企业版安装操作的集群节点，执行以下命令创建存储插件配置文件：\nvi csi-qingcloud.yaml 将以下信息添加到配置文件中，并保存文件供后续安装 KubeSphere 企业版时使用：\nconfig: qy_access_key_id: \u0026#34;\u0026lt;key ID\u0026gt;\u0026#34; qy_secret_access_key: \u0026#34;\u0026lt;access key\u0026gt;\u0026#34; zone: \u0026#34;\u0026lt;zone ID\u0026gt;\u0026#34; sc: isDefaultClass: true 将以下参数替换为实际值：\n参数 描述 \u0026lt;key ID\u0026gt;\nAPI 密钥的 ID。\n\u0026lt;access key\u0026gt;\nAPI 密钥的私钥文本。\n\u0026lt;zone ID\u0026gt;\n云环境的可用区 ID。可用区 ID 将决定 KubeSphere 企业版创建的存储设备所在的区域。ID 取值和可用区的对应关系如下：\nsh1a/sh1b：上海 1 区-A/上海 1 区-B\npek3a/pek3b/pek3c/pek3d：北京 3 区-A/北京 3 区-B/北京 3 区-C/北京 3 区-D\ngd2a/gd2b：广东 2 区-A/广东 2 区-B\nap2a：亚太 2 区-A\n以上配置文件仅包含必须设置的参数。如需设置其他参数，请参阅 QingCloud CSI 配置。\n","href":"/v4.1.3/03-installation-and-upgrade/01-preparations/04-configure-external-persistent-storage/01-configure-storage-devices-on-cloud/","isSection":null,"linkkey":null,"title":"配置云上存储设备"},{"content":" 本节介绍如何添加 KubeSphere 企业版集群节点。\n节点添加过程中将用到开源工具 KubeKey。有关 KubeKey 的更多信息，请访问 GitHub KubeKey 仓库。\n说明 本节介绍的节点添加方式仅适用于 Kubernetes 通过 KubeKey 安装的场景。如果您的 Kubernetes 不是通过 KubeKey 安装，请参阅 Kubernetes 官方文档添加节点。\n前提条件 您需要联系 KubeSphere 企业版的交付服务专家获取 KubeSphere 企业版 v4.1.3 安装包。\n新增节点的操作系统和版本须为 Ubuntu 16.04、Ubuntu 18.04、Ubuntu 20.04、Ubuntu 22.04、Debian 9、Debian 10、CentOS 7、CentOS Stream、RHEL 7、RHEL 8、SLES 15 或 openSUSE Leap 15。多个集群节点的操作系统可以不同。关于其它操作系统和版本支持，请咨询青云科技官方解决方案专家或交付服务专家。\n为确保集群具有足够的计算和存储资源，建议新增节点配置至少 8 个 CPU 核心，16 GB 内存和 200 GB 磁盘空间。除此之外，建议在每台集群节点的 /var/lib/docker（对于 Docker）或 /var/lib/containerd（对于 containerd） 目录额外挂载至少 200 GB 磁盘空间用于存储容器运行时数据。\n如果添加控制平面节点，您需要提前为集群配置高可用性。如果您使用负载均衡器，请确保负载均衡器监听所有控制平面节点的 6443 端口。有关更多信息，请参阅配置高可用性。\n如果您的集群节点无法连接互联网，您还需要准备一台 Linux 服务器用于创建私有镜像服务，该服务器必须与 KubeSphere 企业版集群节点网络连通并且在 /mnt/registry 目录挂载至少 100 GB 磁盘空间。\n您需要获取安装配置文件 config-sample.yaml 并将其传输到用于执行本节操作的集群节点。有关更多信息，请参阅安装 Kubernetes 和 KubeSphere 企业版。\n警告 添加节点过程中不支持在 config-sample.yaml 配置文件中修改原有的集群配置。\n如果您无法获取安装配置文件 config-sample.yaml，您需要参阅安装 Kubernetes 和 KubeSphere 企业版 重新创建 config-sample.yaml 文件。重新创建该文件时，请务必确保文件中的集群信息与集群的当前实际情况一致。否则，添加节点后集群可能会出现错误。\n操作步骤 将 KubeSphere 企业版安装包传输到任意集群节点，并登录该集群节点。\n执行以下命令解压安装包，并进入安装包解压后生成的目录（将 \u0026lt;package name\u0026gt; 替换为安装包的实际名称，将 \u0026lt;directory\u0026gt; 替换为安装包解压后生成的目录）：\ntar -zxvf \u0026lt;package name\u0026gt; cd \u0026lt;directory\u0026gt; 执行以下命令为 KubeKey 二进制文件 kk 添加执行权限：\nsudo chmod +x kk 将安装配置文件 config-sample.yaml 传输到当前目录。\n执行以下命令编辑安装配置文件 config-sample.yaml：\nvi config-sample.yaml 在 config-sample.yaml 文件的 hosts 参数下设置新增节点的信息。\n参数 描述 name\n用户自定义的服务器名称。\naddress\n服务器的 SSH 登录 IP 地址。\ninternalAddress\n服务器在子网内部的 IP 地址。\nport\n服务器的 SSH 端口号。如果使用默认端口 22 可不设置此参数。\nuser\n服务器的 SSH 登录用户名，该用户必须为 root 用户或其他具有 sudo 命令执行权限的用户。如果使用 root 用户可不设置此参数。\npassword\n服务器的 SSH 登录密码。如果已经设置 privateKeyPath 可不设置此参数。\nprivateKeyPath\n服务器的 SSH 登录密钥的路径。如果已经设置 password 可不设置此参数。\narch\n服务器的硬件架构。如果服务器的硬件架构为 arm64，请将此参数设置为 arm64，否则请勿设置此参数。安装包默认仅支持所有集群节点都为 x86_64 或 arm64 架构的场景。如果某集群节点的硬件架构不完全相同，需针对具体情况进行技术评估，请咨询青云科技官方解决方案专家或交付服务专家。\n警告 请勿修改原有节点的信息。否则，添加节点后集群可能会出现错误。\n在 config-sample.yaml 文件的 roleGroups 参数下设置新增节点在集群中的角色。\n参数 描述 etcd\n安装 etcd 数据库的节点。请在此参数下设置集群控制平面节点。\ncontrol-plane\n集群控制平面节点。如果您已经为集群配置了高可用性，您可以设置多个控制平面节点。\nworker\n集群工作节点。\nregistry\n用于创建私有镜像服务的服务器。该服务器不会用作集群节点。 安装、升级 KubeSphere 企业版时，如果集群节点无法连接互联网，需要在此参数下设置用于创建私有镜像服务的服务器。其他情况下请将此参数注释掉。\n警告 请勿修改原有节点的角色。否则，添加节点后集群可能会出现错误。\n如果新增控制平面节点并且当前集群未配置高可用性，在 config-sample.yaml 文件的 controlPlaneEndpoint 参数下设置高可用性信息。\n参数 描述 internalLoadbalancer\n本地负载均衡器的类型。如果使用本地负载均衡配置，请将此参数设置为 haproxy。否则，请将此参数注释掉。\ndomain\n负载均衡器的内部访问域名。请将此参数设置为 lb.kubesphere.local。\naddress\n负载均衡器的 IP 地址。如果使用本地负载均衡配置，请将此参数留空；如果使用专用负载均衡器，请将此参数设置为负载均衡器的 IP 地址；如果使用通用服务器作为负载均衡器，请将此参数设置为负载均衡器的浮动 IP 地址。\nport\n负载均衡器监听的端口号，即 apiserver 服务的端口号。请将此参数设置为 6443。\n警告 如果当前集群已配置高可用性，请勿修改 config-sample.yaml 文件中的高可用性信息。否则，添加节点后集群可能会出现错误。\n如果当前集群使用本地负载均衡实现高可用性，您不需要对集群高可用性进行任何操作；如果当前集群使用负载均衡器实现高可用性，您只需要设置负载均衡器监听所有控制平面节点的 6443 端口。有关更多信息，请参阅配置高可用性。\n保存配置文件，执行以下命令开始添加节点：\n./kk add nodes -f config-sample.yaml -a kubekey-artifact.tar.gz 执行以下命令查看当前集群的节点：\nkubectl get node 如果显示新增节点的信息，则表明节点添加成功。\n","href":"/v4.1.3/03-installation-and-upgrade/05-add-and-delete-cluster-nodes/01-add-cluster-nodes/","isSection":null,"linkkey":null,"title":"添加集群节点"},{"content":" 本节介绍如何在可访问 Internet 的环境下安装 Kubernetes 和 KubeSphere 企业版。\n安装过程中将用到开源工具 KubeKey。有关 KubeKey 的更多信息，请访问 GitHub KubeKey 仓库。\n前提条件 您需要准备至少 1 台 Linux 服务器作为集群节点。在生产环境中，为确保集群具备高可用性，建议准备至少 5 台 Linux 服务器，其中 3 台作为控制平面节点，另外 2 台作为工作节点。如果您在多台 Linux 服务器上安装 KubeSphere 企业版，请确保所有服务器属于同一子网。\n集群节点的操作系统和版本须为 Ubuntu 16.04、Ubuntu 18.04、Ubuntu 20.04、Ubuntu 22.04、Debian 9、Debian 10、CentOS 7、CentOS Stream、RHEL 7、RHEL 8、SLES 15 或 openSUSE Leap 15。多台服务器的操作系统可以不同。关于其它操作系统和版本支持，请咨询青云科技官方解决方案专家或交付服务专家。\n在生产环境中，为确保集群具有足够的计算和存储资源，建议每台集群节点配置至少 8 个 CPU 核心、16 GB 内存和 200 GB 磁盘空间。除此之外，建议在每台集群节点的 /var/lib/docker（对于 Docker）或 /var/lib/containerd（对于 containerd） 目录额外挂载至少 200 GB 磁盘空间用于存储容器运行时数据。\n在生产环境中，建议提前为 KubeSphere 企业版集群配置高可用性以避免单个控制平面节点出现故障时集群服务中断。有关更多信息，请参阅配置高可用性。\n说明 如果您规划了多个控制平面节点，请务必提前为集群配置高可用性。\n默认情况下，KubeSphere 企业版使用集群节点的本地磁盘空间作为持久化存储。在生产环境中，建议提前配置外部存储系统作为持久化存储。有关更多信息，请参阅配置外部持久化存储。\n如果集群节点未安装容器运行时，安装工具 KubeKey 将在安装过程中自动为每个集群节点安装 Docker 作为容器运行时。您也可以提前手动安装 containerd、CRI-O 或 iSula 作为容器运行时。\n说明 CRI-O 和 iSula 与 KubeSphere 企业版的兼容性尚未经过充分测试，可能存在未知问题。\n请确保所有集群节点上 /etc/resolv.conf 文件中配置的 DNS 服务器地址可用。否则，KubeSphere 企业版集群可能会出现域名解析问题。\n请确保在所有集群节点上都可以使用 sudo、curl 和 openssl 命令。\n请确保所有集群节点时间同步。\n配置防火墙规则 KubeSphere 企业版需要特定端口和协议用于服务之间的通信。如果您的基础设施环境已启用防火墙，您需要在防火墙设置中放行所需的端口和协议。如果您的基础设施环境未启用防火墙，您可以跳过此步骤。\n下表列出需要在防火墙中放行的端口和协议。\n服务 协议 起始端口 结束端口 备注 ssh\nTCP\n22\netcd\nTCP\n2379\n2380\napiserver\nTCP\n6443\ncalico\nTCP\n9099\n9100\nbgp\nTCP\n179\nnodeport\nTCP\n30000\n32767\nmaster\nTCP\n10250\n10258\ndns\nTCP\n53\ndns\nUDP\n53\nmetrics-server\nTCP\n8443\nlocal-registry\nTCP\n5000\n离线环境需要\nlocal-apt\nTCP\n5080\n离线环境需要\nrpcbind\nTCP\n111\n使用 NFS 作为持久化存储时需要\nipip\nIPENCAP/IPIP\n使用 Calico 时需要\n安装依赖项 您需要为所有集群节点安装 socat、conntrack、ebtables 和 ipset。如果上述依赖项在各集群节点上已存在，您可以跳过此步骤。\n在 Ubuntu 操作系统上，执行以下命令为服务器安装依赖项：\nsudo apt install socat conntrack ebtables ipset -y 如果集群节点使用其他操作系统，请将 apt 替换为操作系统对应的软件包管理工具。\n创建 Kubernetes 集群 如果您访问 GitHub/Googleapis 受限，请登录任意集群节点，执行以下命令设置下载区域：\nexport KKZONE=cn 执行以下命令下载 KubeKey 最新版本：\ncurl -sfL https://get-kk.kubesphere.io | sh - 下载完成后当前目录下将生成 KubeKey 二进制文件 kk。\n执行以下命令为 KubeKey 二进制文件 kk 添加执行权限：\nsudo chmod +x kk 执行以下命令创建安装配置文件 config-sample.yaml：\n./kk create config --with-kubernetes \u0026lt;Kubernetes version\u0026gt; 将 \u0026lt;Kubernetes version\u0026gt; 替换为实际需要的版本，例如 v1.27.4。KubeSphere 企业版默认支持 Kubernetes v1.21~1.30。\n命令执行完毕后将生成安装配置文件 config-sample.yaml。\n说明 安装完成后，请勿删除安装配置文件 config-sample.yaml，后续进行节点添加等操作时仍需要使用该文件。如果该文件丢失，您需要重新创建安装配置文件。\n执行以下命令编辑安装配置文件 config-sample.yaml：\nvi config-sample.yaml 以下为部分示例配置文件，如需了解完整示例，请参阅此文件。\napiVersion: kubekey.kubesphere.io/v1alpha2 kind: Cluster metadata: name: sample spec: hosts: - {name: controlplane1, address: 192.168.0.2, internalAddress: 192.168.0.2, port: 23, user: ubuntu, password: Testing123, arch: arm64} # arm64 节点注意添加参数 arch: arm64 - {name: controlplane2, address: 192.168.0.3, internalAddress: 192.168.0.3, user: ubuntu, privateKeyPath: \u0026#34;~/.ssh/id_rsa\u0026#34;} - {name: worker1, address: 192.168.0.4, internalAddress: 192.168.0.4, user: ubuntu, password: Testing123} - {name: worker2, address: 192.168.0.5, internalAddress: 192.168.0.5, user: ubuntu, password: Testing123} - {name: registry, address: 192.168.0.6, internalAddress: 192.168.0.6, user: ubuntu, password: Testing123} roleGroups: etcd: - controlplane1 - controlplane2 control-plane: - controlplane1 - controlplane2 worker: - worker1 - worker2 # 如需使用 kk 自动部署镜像仓库，请设置 registry（建议镜像仓库与集群节点分离部署，减少相互影响） registry: - registry controlPlaneEndpoint: internalLoadbalancer: haproxy # 如需部署⾼可⽤集群，且⽆负载均衡器可⽤，可开启该参数，做集群内部负载均衡 domain: lb.kubesphere.local address: \u0026#34;\u0026#34; port: 6443 kubernetes: version: v1.23.15 clusterName: cluster.local network: plugin: calico kubePodsCIDR: 10.233.64.0/18 kubeServiceCIDR: 10.233.0.0/18 ## multus support. https://github.com/k8snetworkplumbingwg/multus-cni enableMultusCNI: false registry: # 如需使用 kk 部署 harbor，可将该参数设置为 harbor，不设置该参数且需使用 kk 部署容器镜像仓库，将默认部署 docker registry。 # harbor 不支持 arm64，arm64 环境部署时，可不配置该参数。 type: harbor # 如使用 kk 部署的 harbor 或其他需要登录的仓库，需设置对应仓库的 auths，如使用 kk 部署默认的 docker registry 仓库，则无需配置 auths 参数。 # 注意：如使用 kk 部署 harbor，auths 参数请于创建 harbor 项目之后设置。 auths: \u0026#34;dockerhub.kubekey.local\u0026#34;: username: admin # harbor 默认用户名 password: Harbor12345 # harbor 默认密码 plainHTTP: false # 如果仓库使用 http，请将该参数设置为 true privateRegistry: \u0026#34;dockerhub.kubekey.local/kse\u0026#34; # 设置集群部署时使用的私有仓库地址 registryMirrors: [] insecureRegistries: [] addons: [] 在 config-sample.yaml 配置文件的 spec:hosts 参数下设置各服务器的信息。\n参数 描述 name\n用户自定义的服务器名称。\naddress\n服务器的 SSH 登录 IP 地址。\ninternalAddress\n服务器在子网内部的 IP 地址。\nport\n服务器的 SSH 端口号。如果使用默认端口 22 可不设置此参数。\nuser\n服务器的 SSH 登录用户名，该用户必须为 root 用户或其他具有 sudo 命令执行权限的用户。如果使用 root 用户可不设置此参数。\npassword\n服务器的 SSH 登录密码。如果已经设置 privateKeyPath 可不设置此参数。\nprivateKeyPath\n服务器的 SSH 登录密钥的路径。如果已经设置 password 可不设置此参数。\narch\n服务器的硬件架构。如果服务器的硬件架构为 arm64，请将此参数设置为 arm64，否则请勿设置此参数。安装包默认仅支持所有集群节点都为 x86_64 或 arm64 架构的场景。如果某集群节点的硬件架构不完全相同，需针对具体情况进行技术评估，请咨询青云科技官方解决方案专家或交付服务专家。\n在 config-sample.yaml 配置文件的 spec:roleGroups 参数下设置服务器的角色：\n参数 描述 etcd\n安装 etcd 数据库的节点。请在此参数下设置集群控制平面节点。\ncontrol-plane\n集群控制平面节点。如果您已经为集群配置了高可用性，您可以设置多个控制平面节点。\nworker\n集群工作节点。\nregistry\n用于创建私有镜像服务的服务器。该服务器不会用作集群节点。 安装、升级 KubeSphere 企业版时，如果集群节点无法连接互联网，需要在此参数下设置用于创建私有镜像服务的服务器。其他情况下请将此参数注释掉。\n如果您规划了多个控制平面节点，在 config-sample.yaml 配置文件的 spec:controlPlaneEndpoint 参数下设置高可用性信息。\n参数 描述 internalLoadbalancer\n本地负载均衡器的类型。如果使用本地负载均衡配置，请将此参数设置为 haproxy。否则，请将此参数注释掉。\ndomain\n负载均衡器的内部访问域名。请将此参数设置为 lb.kubesphere.local。\naddress\n负载均衡器的 IP 地址。如果使用本地负载均衡配置，请将此参数留空；如果使用专用负载均衡器，请将此参数设置为负载均衡器的 IP 地址；如果使用通用服务器作为负载均衡器，请将此参数设置为负载均衡器的浮动 IP 地址。\nport\n负载均衡器监听的端口号，即 apiserver 服务的端口号。请将此参数设置为 6443。\n如果您需要使用外部持久化存储，在 config-sample.yaml 配置文件的 spec:addons 参数下设置外部持久化存储信息。\n如果使用云上存储设备，在 spec:addons 下设置以下参数（将 \u0026lt;configuration file path\u0026gt; 替换为存储插件配置文件的实际路径）：\n- name: csi-qingcloud namespace: kube-system sources: chart: name: csi-qingcloud repo: https://charts.kubesphere.io/test valuesFile: \u0026lt;configuration file path\u0026gt; 如果使用 NeonSAN 存储设备，在 spec:addons 下设置以下参数（将 \u0026lt;configuration file path\u0026gt; 替换为存储插件配置文件的实际路径）：\n- name: csi-neonsan namespace: kube-system sources: chart: name: csi-neonsan repo: https://charts.kubesphere.io/test valuesFile: \u0026lt;configuration file path\u0026gt; 如果使用 NFS 存储系统，在 spec:addons 下设置以下参数（将 \u0026lt;configuration file path\u0026gt; 替换为存储插件配置文件的实际路径）：\n- name: nfs-client namespace: kube-system sources: chart: name: nfs-client-provisioner repo: https://charts.kubesphere.io/main valuesFile: \u0026lt;configuration file path\u0026gt; 执行以下命令创建 Kubernetes 集群：\n./kk create cluster -f config-sample.yaml 说明 如需使用 openebs localpv，可在命令后添加参数 --with-local-storage。如需对接其他存储，可在配置文件 addons 中添加配置相关存储插件，或 Kubernetes 集群部署完成后自行安装。\n如果显示如下信息，则表明 Kubernetes 集群创建成功。\nPipeline[CreateclusterPipeline] execute successfully 安装 KubeSphere 企业版 KubeSphere Core (ks-core) 是 KubeSphere 企业版的核心组件，为扩展组件提供基础的运行环境。KubeSphere Core 安装完成后，即可访问 KubeSphere 企业版 Web 控制台。\n在集群节点，执行以下命令安装 KubeSphere Core。\nchart=oci://hub.kubesphere.com.cn/kse/ks-core version=1.1.1 helm upgrade --install -n kubesphere-system --create-namespace ks-core $chart --version $version --debug --wait 如果显示如下信息，则表明 ks-core 安装成功：\nNOTES: Thank you for choosing KubeSphere Helm Chart. Please be patient and wait for several seconds for the KubeSphere deployment to complete. 1. Wait for Deployment Completion Confirm that all KubeSphere components are running by executing the following command: kubectl get pods -n kubesphere-system 2. Access the KubeSphere Console Once the deployment is complete, you can access the KubeSphere console using the following URL: http://192.168.6.10:30880 3. Login to KubeSphere Console Use the following credentials to log in: Account: admin Password: P@88w0rd NOTE: It is highly recommended to change the default password immediately after the first login. For additional information and details, please visit https://kubesphere.io. 从成功信息中的 Console、Account 和 Password 参数分别获取 KubeSphere 企业版 Web 控制台的 IP 地址、管理员用户名和管理员密码，并使用网页浏览器登录 KubeSphere 企业版 Web 控制台。\n说明 取决于您的网络环境，您可能需要配置流量转发规则并在防火墙中放行 30880 端口。\n此时，Web 控制台仅提供 KubeSphere 企业版的核心功能，若要体验更多功能，还需在扩展中心安装扩展组件。\n在使用 KubeSphere 企业版及扩展组件之前，请先激活 KubeSphere 企业版和扩展组件。\n","href":"/v4.1.3/03-installation-and-upgrade/02-install-kubesphere/01-online-install-kubernetes-and-kubesphere/","isSection":null,"linkkey":null,"title":"在线安装 Kubernetes 和 KubeSphere 企业版"},{"content":" 本节介绍安装 KubeSphere 企业版前需要进行的准备工作。\n","href":"/v4.1.3/03-installation-and-upgrade/01-preparations/","isSection":null,"linkkey":null,"title":"准备工作"},{"content":" 本节介绍如何安装 Kubernetes 和 KubeSphere 企业版。\n","href":"/v4.1.3/03-installation-and-upgrade/02-install-kubesphere/","isSection":null,"linkkey":null,"title":"安装{ks_product_left}"},{"content":" 本节介绍如何离线安装 Kubernetes 和 KubeSphere 企业版。部署环境无需访问外网，可完全离线安装 KubeSphere 企业版和扩展组件。\n安装过程中将用到开源工具 KubeKey。有关 KubeKey 的更多信息，请访问 GitHub KubeKey 仓库。\n前提条件 您需要联系 KubeSphere 企业版的交付服务专家获取 KubeSphere 企业版 v4.1.3 安装包。\n您需要准备至少 1 台 Linux 服务器作为集群节点。在生产环境中，为确保集群具备高可用性，建议准备至少 5 台 Linux 服务器，其中 3 台作为控制平面节点，另外 2 台作为工作节点。如果您在多台 Linux 服务器上安装 KubeSphere 企业版，请确保所有服务器属于同一子网。\n集群节点的操作系统和版本须为 Ubuntu 16.04、Ubuntu 18.04、Ubuntu 20.04、Ubuntu 22.04、Debian 9、Debian 10、CentOS 7、CentOS Stream、RHEL 7、RHEL 8、SLES 15 或 openSUSE Leap 15。多台服务器的操作系统可以不同。关于其它操作系统和版本支持，请咨询青云科技官方解决方案专家或交付服务专家。\n在生产环境中，为确保集群具有足够的计算和存储资源，建议每台集群节点配置至少 8 个 CPU 核心、16 GB 内存和 200 GB 磁盘空间。除此之外，建议在每台集群节点的 /var/lib/docker（对于 Docker）或 /var/lib/containerd（对于 containerd） 目录额外挂载至少 200 GB 磁盘空间用于存储容器运行时数据。\n除集群节点外，您还需要准备一台 Linux 服务器用于创建私有镜像服务，该服务器必须与 KubeSphere 企业版集群节点网络连通，并且在 /mnt/registry 目录挂载至少 100 GB 磁盘空间。\n在生产环境中，建议提前为 KubeSphere 企业版集群配置高可用性以避免单个控制平面节点出现故障时集群服务中断。有关更多信息，请参阅配置高可用性。\n说明 如果您规划了多个控制平面节点，请务必提前为集群配置高可用性。\n默认情况下，KubeSphere 企业版使用集群节点的本地磁盘空间作为持久化存储。在生产环境中，建议提前配置外部存储系统作为持久化存储。有关更多信息，请参阅配置外部持久化存储。\n如果集群节点未安装容器运行时，安装工具 KubeKey 将在安装过程中自动为每个集群节点安装 Docker 作为容器运行时。您也可以提前手动安装 containerd、CRI-O 或 iSula 作为容器运行时。\n说明 CRI-O 和 iSula 与 KubeSphere 企业版的兼容性尚未经过充分测试，可能存在未知问题。\n请确保所有集群节点上 /etc/resolv.conf 文件中配置的 DNS 服务器地址可用。否则，KubeSphere 企业版集群可能会出现域名解析问题。\n请确保在所有集群节点上都可以使用 sudo、curl 和 openssl 命令。\n请确保所有集群节点时间同步。\n配置防火墙规则 KubeSphere 企业版需要特定端口和协议用于服务之间的通信。如果您的基础设施环境已启用防火墙，您需要在防火墙设置中放行所需的端口和协议。如果您的基础设施环境未启用防火墙，您可以跳过此步骤。\n下表列出需要在防火墙中放行的端口和协议。\n服务 协议 起始端口 结束端口 备注 ssh\nTCP\n22\netcd\nTCP\n2379\n2380\napiserver\nTCP\n6443\ncalico\nTCP\n9099\n9100\nbgp\nTCP\n179\nnodeport\nTCP\n30000\n32767\nmaster\nTCP\n10250\n10258\ndns\nTCP\n53\ndns\nUDP\n53\nmetrics-server\nTCP\n8443\nlocal-registry\nTCP\n5000\n离线环境需要\nlocal-apt\nTCP\n5080\n离线环境需要\nrpcbind\nTCP\n111\n使用 NFS 作为持久化存储时需要\nipip\nIPENCAP/IPIP\n使用 Calico 时需要\n安装依赖项 您需要为所有集群节点安装 socat、conntrack、ebtables 和 ipset。如果上述依赖项在各集群节点上已存在，您可以跳过此步骤。\n在 Ubuntu 操作系统上，执行以下命令为服务器安装依赖项：\nsudo apt install socat conntrack ebtables ipset -y 如果集群节点使用其他操作系统，请将 apt 替换为操作系统对应的软件包管理工具。\n校验安装包完整性 获取 KubeSphere 企业版安装包后，可采用校验文件哈希值的方法验证安装包的完整性，以确保其没有被损坏。\n根据安装包文件所在的操作系统，执行对应的命令，查看安装包的 SHA256 值。\nWindows 系统\ncertutil -hashfile [file location] SHA256 示例：\ncertutil -hashfile C:\\Users\\user1\\Downloads\\kse-v4.1.3-offline-linux-amd64-all-20250225.tar.gz SHA256 Linux 系统\nsha256sum [file location] 示例：\nsha256sum ~/Downloads/kse-v4.1.3-offline-linux-amd64-all-20250225.tar.gz MacOS 系统\nshasum -a 256 [file location] 示例：\nshasum -a 256 ~/Downloads/kse-v4.1.3-offline-linux-amd64-all-20250225.tar.gz 与安装包中的 SHA256 值进行比对，确保上一步得到的 SHA256 与其一致。\n注意 如果 SHA256 不一致，您需要重新获取完整的 KubeSphere 企业版安装包。\n查看安装包内容 了解 KubeSphere 企业版 v4.1.3 的安装包内容，以便进行后续步骤。\n安装包包含以下文件：\nkse-v4.1.3-offline-linux-amd64-all/ ├── charts │ ├── ks-core # KubeSphere 企业版核心组件 │ └── nfs-client-provisioner # 用于对接 NFS 存储 ├── tools │ ├── extension-resources-patch.sh # 用于处理 v4.1.2 及之前版本通过 kse-extension-publish 创建的扩展组件资源产生的冲突 │ └── oras # OCI 工具，便于镜像同步等操作 ├── kse-extensions # 其中包含所有扩展组件的 installplan，可用于快速安装 KubeSphere 企业版扩展组件 ├── config-sample.yaml # 安装配置文件的模版 ├── create_project_harbor.sh # 用于快速创建 harbor 项目 ├── kk # 集群部署工具 ├── kubekey-artifact.tar.gz # KubeSphere 企业版制品，其中包含集群部署所需的二进制文件及镜像 └── manifest-v413-amd64.yaml # KubeSphere 企业版制品清单，其中包含各组件的版本以及镜像列表 配置安装配置文件 config-sample.yaml 是 KubeSphere 企业版的安装配置文件，请先配置该文件，以便进行后续步骤。\n说明 安装完成后，请勿删除安装配置文件 config-sample.yaml，后续进行节点添加等操作时仍需要使用该文件。如果该文件丢失，您需要重新创建安装配置文件。\n将 KubeSphere 企业版安装包传输到任意集群节点，并登录该集群节点。\n执行以下命令解压安装包，并进入安装包解压后生成的目录（将 \u0026lt;package name\u0026gt; 替换为安装包的实际名称，将 \u0026lt;directory\u0026gt; 替换为安装包解压后生成的目录）：\ntar -zxvf \u0026lt;package name\u0026gt; cd \u0026lt;directory\u0026gt; 执行以下命令为 KubeKey 二进制文件 kk 添加执行权限：\nsudo chmod +x kk 执行以下命令编辑安装配置文件 config-sample.yaml：\nvi config-sample.yaml 以下为部分示例配置文件，如需了解完整示例，请参阅此文件。\napiVersion: kubekey.kubesphere.io/v1alpha2 kind: Cluster metadata: name: sample spec: hosts: - {name: controlplane1, address: 192.168.0.2, internalAddress: 192.168.0.2, port: 23, user: ubuntu, password: Testing123, arch: arm64} # arm64 节点注意添加参数 arch: arm64 - {name: controlplane2, address: 192.168.0.3, internalAddress: 192.168.0.3, user: ubuntu, privateKeyPath: \u0026#34;~/.ssh/id_rsa\u0026#34;} - {name: worker1, address: 192.168.0.4, internalAddress: 192.168.0.4, user: ubuntu, password: Testing123} - {name: worker2, address: 192.168.0.5, internalAddress: 192.168.0.5, user: ubuntu, password: Testing123} - {name: registry, address: 192.168.0.6, internalAddress: 192.168.0.6, user: ubuntu, password: Testing123} roleGroups: etcd: - controlplane1 - controlplane2 control-plane: - controlplane1 - controlplane2 worker: - worker1 - worker2 # 如需使用 kk 自动部署镜像仓库，请设置 registry（建议镜像仓库与集群节点分离部署，减少相互影响） registry: - registry controlPlaneEndpoint: internalLoadbalancer: haproxy # 如需部署⾼可⽤集群，且⽆负载均衡器可⽤，可开启该参数，做集群内部负载均衡 domain: lb.kubesphere.local address: \u0026#34;\u0026#34; port: 6443 kubernetes: version: v1.23.15 clusterName: cluster.local network: plugin: calico kubePodsCIDR: 10.233.64.0/18 kubeServiceCIDR: 10.233.0.0/18 ## multus support. https://github.com/k8snetworkplumbingwg/multus-cni enableMultusCNI: false registry: # 如需使用 kk 部署 harbor，可将该参数设置为 harbor，不设置该参数且需使用 kk 部署容器镜像仓库，将默认部署 docker registry。 # harbor 不支持 arm64，arm64 环境部署时，可不配置该参数。 type: harbor # 如使用 kk 部署的 harbor 或其他需要登录的仓库，需设置对应仓库的 auths，如使用 kk 部署默认的 docker registry 仓库，则无需配置 auths 参数。 # 注意：如使用 kk 部署 harbor，auths 参数请于创建 harbor 项目之后设置。 auths: \u0026#34;dockerhub.kubekey.local\u0026#34;: username: admin # harbor 默认用户名 password: Harbor12345 # harbor 默认密码 plainHTTP: false # 如果仓库使用 http，请将该参数设置为 true privateRegistry: \u0026#34;dockerhub.kubekey.local/kse\u0026#34; # 设置集群部署时使用的私有仓库地址 registryMirrors: [] insecureRegistries: [] addons: [] 在 config-sample.yaml 配置文件的 spec:hosts 参数下设置各服务器的信息。\n参数 描述 name\n用户自定义的服务器名称。\naddress\n服务器的 SSH 登录 IP 地址。\ninternalAddress\n服务器在子网内部的 IP 地址。\nport\n服务器的 SSH 端口号。如果使用默认端口 22 可不设置此参数。\nuser\n服务器的 SSH 登录用户名，该用户必须为 root 用户或其他具有 sudo 命令执行权限的用户。如果使用 root 用户可不设置此参数。\npassword\n服务器的 SSH 登录密码。如果已经设置 privateKeyPath 可不设置此参数。\nprivateKeyPath\n服务器的 SSH 登录密钥的路径。如果已经设置 password 可不设置此参数。\narch\n服务器的硬件架构。如果服务器的硬件架构为 arm64，请将此参数设置为 arm64，否则请勿设置此参数。安装包默认仅支持所有集群节点都为 x86_64 或 arm64 架构的场景。如果某集群节点的硬件架构不完全相同，需针对具体情况进行技术评估，请咨询青云科技官方解决方案专家或交付服务专家。\n在 config-sample.yaml 配置文件的 spec:roleGroups 参数下设置服务器的角色：\n参数 描述 etcd\n安装 etcd 数据库的节点。请在此参数下设置集群控制平面节点。\ncontrol-plane\n集群控制平面节点。如果您已经为集群配置了高可用性，您可以设置多个控制平面节点。\nworker\n集群工作节点。\nregistry\n用于创建私有镜像服务的服务器。该服务器不会用作集群节点。 安装、升级 KubeSphere 企业版时，如果集群节点无法连接互联网，需要在此参数下设置用于创建私有镜像服务的服务器。其他情况下请将此参数注释掉。\n如果您规划了多个控制平面节点，在 config-sample.yaml 配置文件的 spec:controlPlaneEndpoint 参数下设置高可用性信息。\n参数 描述 internalLoadbalancer\n本地负载均衡器的类型。如果使用本地负载均衡配置，请将此参数设置为 haproxy。否则，请将此参数注释掉。\ndomain\n负载均衡器的内部访问域名。请将此参数设置为 lb.kubesphere.local。\naddress\n负载均衡器的 IP 地址。如果使用本地负载均衡配置，请将此参数留空；如果使用专用负载均衡器，请将此参数设置为负载均衡器的 IP 地址；如果使用通用服务器作为负载均衡器，请将此参数设置为负载均衡器的浮动 IP 地址。\nport\n负载均衡器监听的端口号，即 apiserver 服务的端口号。请将此参数设置为 6443。\n如果您需要使用外部持久化存储，在 config-sample.yaml 配置文件的 spec:addons 参数下设置外部持久化存储信息。\n如果使用云上存储设备，在 spec:addons 下设置以下参数（将 \u0026lt;configuration file path\u0026gt; 替换为存储插件配置文件的实际路径）：\n- name: csi-qingcloud namespace: kube-system sources: chart: name: csi-qingcloud path: charts/csi-qingcloud # 替换为 chart 包的实际路径 valuesFile: \u0026lt;configuration file path\u0026gt; 如果使用 NeonSAN 存储设备，在 spec:addons 下设置以下参数（将 \u0026lt;configuration file path\u0026gt; 替换为存储插件配置文件的实际路径）：\n- name: csi-neonsan namespace: kube-system sources: chart: name: csi-neonsan path: charts/csi-neonsan # 替换为 chart 包的实际路径 valuesFile: \u0026lt;configuration file path\u0026gt; 如果使用 NFS 存储系统，在 spec:addons 下设置以下参数（将 \u0026lt;configuration file path\u0026gt; 替换为存储插件配置文件的实际路径）：\n- name: nfs-client namespace: kube-system sources: chart: name: nfs-client-provisioner repo: charts/nfs-client-provisioner valuesFile: \u0026lt;configuration file path\u0026gt; 创建私有镜像仓库 注意 如果您已有可用的镜像仓库，可跳过此步骤。但需要把私有镜像服务的默认地址 dockerhub.kubekey.local/kse 替换为您的实际镜像仓库地址。\n在配置文件 config-sample.yaml 的 spec:hosts 参数下设置用于创建私有镜像服务的服务器的信息。\nspec: hosts: - {name: registry, address: 192.168.0.6, internalAddress: 192.168.0.6, user: ubuntu, password: Testing123} 参数 描述 name\n用户自定义的服务器名称。\naddress\n服务器的 SSH 登录 IP 地址。\ninternalAddress\n服务器在子网内部的 IP 地址。\nport\n服务器的 SSH 端口号。如果使用默认端口 22 可不设置此参数。\nuser\n服务器的 SSH 登录用户名，该用户必须为 root 用户或其他具有 sudo 命令执行权限的用户。如果使用 root 用户可不设置此参数。\npassword\n服务器的 SSH 登录密码。如果已经设置 privateKeyPath 可不设置此参数。\nprivateKeyPath\n服务器的 SSH 登录密钥的路径。如果已经设置 password 可不设置此参数。\narch\n服务器的硬件架构。如果服务器的硬件架构为 arm64，请将此参数设置为 arm64，否则请勿设置此参数。安装包默认仅支持所有集群节点都为 x86_64 或 arm64 架构的场景。如果某集群节点的硬件架构不完全相同，需针对具体情况进行技术评估，请咨询青云科技官方解决方案专家或交付服务专家。\n在 spec:roleGroups:registry 参数下设置用于创建私有镜像服务的服务器名称（将 \u0026lt;registry name\u0026gt; 替换为 spec:hosts 参数下设置的服务器实际名称）。\nspec: roleGroups: registry: - \u0026lt;registry name\u0026gt; 将 spec:registry:privateRegistry 参数设置为私有镜像服务的默认地址 dockerhub.kubekey.local/kse，然后保存文件。\nspec: registry: registryMirrors: [] insecureRegistries: [] privateRegistry: dockerhub.kubekey.local/kse 执行以下命令初始化私有镜像服务：\n./kk init registry -f config-sample.yaml -a kubekey-artifact.tar.gz 如果显示如下信息，则表明镜像仓库创建成功。\n说明 KubeKey 将在 config-sample.yaml 配置文件中 spec:roleGroups:registry 参数指定的服务器上创建私有镜像服务，默认地址为 dockerhub.kubekey.local/kse。\n若 spec:registry:type 参数设置为 harbor，执行以下命令创建 Harbor 项目。\nbash create_project_harbor.sh 创建 harbor 项目后，在 config-sample.yaml 中配置 spec:registry:auths 参数。\n说明 harbor 安装文件在 /opt/harbor 目录下，可在该目录下对 harbor 进行运维。\n如果同时满足以下条件，需执行以下步骤在 harbor 所在节点添加 containerd 配置。\n镜像仓库使用 harbor\n容器运行时为 containerd\nharbor 所在节点也在待部署的集群中\nmkdir /etc/containerd # 创建 containerd 配置文件 # 注意修改 sandbox_image 以及 [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.configs] # 下的仓库信息为实际环境信息 cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/containerd/config.toml version = 2 root = \u0026#34;/var/lib/containerd\u0026#34; state = \u0026#34;/run/containerd\u0026#34; [grpc] address = \u0026#34;/run/containerd/containerd.sock\u0026#34; uid = 0 gid = 0 max_recv_message_size = 16777216 max_send_message_size = 16777216 [ttrpc] address = \u0026#34;\u0026#34; uid = 0 gid = 0 [debug] address = \u0026#34;\u0026#34; uid = 0 gid = 0 level = \u0026#34;\u0026#34; [metrics] address = \u0026#34;\u0026#34; grpc_histogram = false [cgroup] path = \u0026#34;\u0026#34; [timeouts] \u0026#34;io.containerd.timeout.shim.cleanup\u0026#34; = \u0026#34;5s\u0026#34; \u0026#34;io.containerd.timeout.shim.load\u0026#34; = \u0026#34;5s\u0026#34; \u0026#34;io.containerd.timeout.shim.shutdown\u0026#34; = \u0026#34;3s\u0026#34; \u0026#34;io.containerd.timeout.task.state\u0026#34; = \u0026#34;2s\u0026#34; [plugins] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc] runtime_type = \u0026#34;io.containerd.runc.v2\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc.options] SystemdCgroup = true [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;] sandbox_image = \u0026#34;dockerhub.kubekey.local/kse/kubesphere/pause:3.6\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.cni] bin_dir = \u0026#34;/opt/cni/bin\u0026#34; conf_dir = \u0026#34;/etc/cni/net.d\u0026#34; max_conf_num = 1 conf_template = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.mirrors] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.mirrors.\u0026#34;docker.io\u0026#34;] endpoint = [\u0026#34;https://registry-1.docker.io\u0026#34;] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.configs] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.configs.\u0026#34;dockerhub.kubekey.local\u0026#34;.auth] username = \u0026#34;admin\u0026#34; password = \u0026#34;harbor2345\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.configs.\u0026#34;dockerhub.kubekey.local\u0026#34;.tls] ca_file = \u0026#34;\u0026#34; cert_file = \u0026#34;\u0026#34; key_file = \u0026#34;\u0026#34; insecure_skip_verify = true EOF # 重启 containerd systemctl restart containerd 登录所有集群节点，执行以下命令编辑 /etc/hosts 文件：\nvi /etc/hosts 在 /etc/hosts 文件中添加以下信息，从而为集群节点配置私有镜像服务的域名解析规则（将 \u0026lt;registry IP address\u0026gt; 替换成私有镜像服务的实际 IP 地址，将私有镜像服务的默认地址 dockerhub.kubekey.local 替换为您的实际镜像仓库地址），然后保存文件：\n\u0026lt;registry IP address\u0026gt; dockerhub.kubekey.local 安装 Kubernetes 说明 如果您已有可用的 Kubernetes 集群，可跳过此步骤。\n安装包中集成了 CentOS 7、Ubuntu 18.04、Ubuntu 20.04、Ubuntu 22.04 依赖包，如使用这些操作系统需要使用 kk 自动安装系统依赖，可在安装命令后添加 --with-packages ; 如使用这些操作系统之外的操作系统或由于依赖问题导致失败，需手动安装相关依赖（conntrack）。\n如需使用 openebs localpv，可在如下命令后添加参数 --with-local-storage，如需对接其他存储，可在配置文件 addons 中添加配置相关存储插件，或 Kubernetes 集群部署完成后自行安装。\n如使用 kk 部署的 harbor，请确保安装 Kubernetes 之前，已创建 harbor 项目，且配置文件 config-sample.yaml 中已配置 spec:registry:auths 参数。\n执行以下命令创建 Kubernetes 集群：\n./kk create cluster -f config-sample.yaml -a kubekey-artifact.tar.gz 如果显示如下信息，则表明 Kubernetes 集群创建成功。\nPipeline[CreateclusterPipeline] execute successfully Installation is complete. 导入镜像到私有镜像仓库 执行以下命令将镜像导入到指定的私有镜像仓库中。\n./kk artifact images push -f config-sample.yaml -a kubekey-artifact.tar.gz 如果显示如下信息，则表明导入成功。\nPipeline[ArtifactImagesPushPipeline] execute successfully 安装 KubeSphere 企业版 在集群节点，执行以下命令安装 KubeSphere Core。\nhelm upgrade --install -n kubesphere-system --create-namespace ks-core charts/ks-core \\ --debug \\ --wait \\ --set global.imageRegistry=dockerhub.kubekey.local/kse \\ --set extension.imageRegistry=dockerhub.kubekey.local/kse 注意 将 global.imageRegistry 和 extension.imageRegistry 的默认地址 dockerhub.kubekey.local/kse 替换为您的实际镜像仓库地址。\n取决于您的硬件和网络环境，您可能需要配置流量转发规则并在防火墙中放行 30880 端口。如果显示如下信息，则表明 ks-core 安装成功：\nNOTES: Thank you for choosing KubeSphere Helm Chart. Please be patient and wait for several seconds for the KubeSphere deployment to complete. 1. Wait for Deployment Completion Confirm that all KubeSphere components are running by executing the following command: kubectl get pods -n kubesphere-system 2. Access the KubeSphere Console Once the deployment is complete, you can access the KubeSphere console using the following URL: http://192.168.6.10:30880 3. Login to KubeSphere Console Use the following credentials to log in: Account: admin Password: P@88w0rd NOTE: It is highly recommended to change the default password immediately after the first login. For additional information and details, please visit https://kubesphere.io. 从成功信息中的 Console、Account 和 Password 参数分别获取 KubeSphere 企业版 Web 控制台的 IP 地址、管理员用户名和管理员密码，并使用网页浏览器登录 KubeSphere 企业版 Web 控制台。\n根据激活提示点击激活，导入 KubeSphere 企业版的 license。\n此时，Web 控制台仅提供 KubeSphere 企业版的核心功能，若要体验更多功能，还需在扩展中心安装扩展组件。\n","href":"/v4.1.3/03-installation-and-upgrade/02-install-kubesphere/02-install-kubernetes-and-kubesphere/","isSection":null,"linkkey":null,"title":"离线安装 Kubernetes 和 KubeSphere 企业版"},{"content":" 本节介绍如何删除 KubeSphere 企业版集群节点。\n节点删除过程中将用到开源工具 KubeKey。有关 KubeKey 的更多信息，请访问 GitHub KubeKey 仓库。\n说明 本节介绍的节点删除方式仅适用于 Kubernetes 通过 KubeKey 安装的场景。如果您的 Kubernetes 不是通过 KubeKey 安装，请参阅 Kubernetes 官方文档删除节点。\n警告 请勿删除控制平面节点，否则集群将出现错误。\n请确保节点删除后，集群中剩余的资源仍然足够运行现有的业务。否则，节点删除后可能会出现业务中断。\n前提条件 您需要联系 KubeSphere 企业版的交付服务专家获取 KubeSphere 企业版 v4.1.3 安装包。\n您需要获取安装配置文件 config-sample.yaml 并将其传输到用于执行本节操作的集群节点。有关更多信息，请参阅安装 Kubernetes 和 KubeSphere 企业版。\n警告 如果您无法获取安装配置文件 config-sample.yaml，您需要参阅安装 Kubernetes 和 KubeSphere 企业版 重新创建 config-sample.yaml 文件。重新创建该文件时，请务必确保文件中的集群信息与集群的当前实际情况一致。否则，删除节点后集群可能会出现错误。\n操作步骤 将 KubeSphere 企业版安装包传输到任意集群节点，并登录该集群节点。\n执行以下命令解压安装包，并进入安装包解压后生成的目录（将 \u0026lt;package name\u0026gt; 替换为安装包的实际名称，将 \u0026lt;directory\u0026gt; 替换为安装包解压后生成的目录）：\ntar -zxvf \u0026lt;package name\u0026gt; cd \u0026lt;directory\u0026gt; 执行以下命令为 KubeKey 二进制文件 kk 添加执行权限：\nsudo chmod +x kk 执行以下命令查看需要删除的节点的名称：\nkubectl get node 执行以下命令将需要删除的节点上运行的容器组驱逐到其他节点（将 \u0026lt;node name\u0026gt; 替换为需要删除的节点的名称）：\nkubectl drain \u0026lt;node name\u0026gt; 将安装配置文件 config-sample.yaml 传输到当前目录。\n执行以下命令开始删除节点：\n./kk delete node \u0026lt;node name\u0026gt; -f config-sample.yaml 执行以下命令查看当前集群节点：\nkubectl get node 如果没有显示已删除节点的信息，则表明节点删除成功。\n","href":"/v4.1.3/03-installation-and-upgrade/05-add-and-delete-cluster-nodes/02-delete-cluster-nodes/","isSection":null,"linkkey":null,"title":"删除集群节点"},{"content":" 本节介绍如何卸载 Kubernetes 和 KubeSphere 企业版。\n说明 本节介绍的 Kubernetes 卸载方式仅适用于 Kubernetes 通过 KubeKey 安装的场景。如果您的 Kubernetes 不是通过 KubeKey 安装，请参阅 Kubernetes 官方文档卸载 Kubernetes。\n警告 此操作将导致 KubeSphere 企业版集群业务中断，并且无法撤销，请谨慎执行此操作。\n前提条件 您需要获取安装配置文件 config-sample.yaml 并将其传输到用于执行本节操作的集群节点。有关更多信息，请参阅安装 Kubernetes 和 KubeSphere 企业版。\n说明 如果您无法获取安装配置文件 config-sample.yaml，您需要参阅安装 Kubernetes 和 KubeSphere 企业版 重新创建 config-sample.yaml 文件。重新创建该文件时，请务必确保文件中的集群信息与集群的当前实际情况一致。否则，卸载过程可能会出现错误。\n为避免数据丢失，请提前备份所有重要数据。\n操作步骤 将 KubeSphere 企业版安装包传输到任意集群节点，并登录该集群节点。\n执行以下命令解压安装包，并进入安装包解压后生成的目录（将 \u0026lt;package name\u0026gt; 替换为安装包的实际名称，将 \u0026lt;directory\u0026gt; 替换为安装包解压后生成的目录）：\ntar -zxvf \u0026lt;package name\u0026gt; cd \u0026lt;directory\u0026gt; 执行以下命令为 KubeKey 二进制文件 kk 添加执行权限：\nsudo chmod +x kk 将安装配置文件 config-sample.yaml 传输到当前目录。\n执行以下命令开始卸载 KubeSphere 企业版：\n./kk delete cluster -f config-sample.yaml 如果显示如下信息，则表明卸载成功：\nPipeline[DeleteClusterPipeline] execute successful ","href":"/v4.1.3/03-installation-and-upgrade/04-uninstall-kubesphere/02-uninstall-kubernetes-and-kubesphere/","isSection":null,"linkkey":null,"title":"卸载 Kubernetes 和{ks_product_left}"},{"content":" 本节介绍如何安装、升级和卸载 KubeSphere 企业版，以及如何添加和删除 KubeSphere 企业版集群节点。\n","href":"/v4.1.3/03-installation-and-upgrade/","isSection":null,"linkkey":null,"title":"安装指南"},{"content":" 登录 KubeSphere 企业版 Web 控制台后，您需要导入许可证（license），激活 KubeSphere 企业版和扩展组件，以体验所有功能。\n说明 若您是离线安装的 KubeSphere 企业版，请联系交付服务专家直接获取 license。\n步骤 1：购买并获取 license 若您是在线安装的 KubeSphere 企业版，请访问 KubeSphere 企业版官网，点击联系我们，通过微信或 slack 咨询 KubeSphere 企业版和扩展组件 license 的购买方式和获取方式。\n步骤 2：导入 license 待获得 license 后，请参阅添加许可证导入 KubeSphere 企业版和扩展组件的 license。\n步骤 3：验证激活 若许可证页面显示 KubeSphere 企业版和扩展组件已授权，即表示激活成功，可开始使用 KubeSphere 企业版。\n","href":"/v4.1.3/03-installation-and-upgrade/02-install-kubesphere/03-activate-kse/","isSection":null,"linkkey":null,"title":"激活 KubeSphere 企业版"},{"content":" 本节介绍如何在生产环境中为 KubeSphere 企业版集群配置多个控制平面节点，以防止单个控制平面节点故障时集群服务中断，从而实现高可用性。如果您的 KubeSphere 企业版集群没有高可用性需求，您可以跳过本节。\n说明 KubeSphere 企业版高可用性配置仅支持同时安装 Kubernetes 和 KubeSphere 企业版的场景。如果您在现有的 Kubernetes 集群上安装 KubeSphere 企业版，KubeSphere 企业版安装完成后将使用 Kubernetes 集群现有的高可用性配置。\n本节介绍以下高可用性配置方式：\n使用本地负载均衡配置。您可以在安装 KubeSphere 企业版的过程中，设置 KubeKey 工具在工作节点上安装 HAProxy 作为各控制平面节点的反向代理，所有工作节点的 Kubernetes 组件将通过 HAProxy 连接各控制平面节点。这种方式需要额外的健康检查机制，所以相较其他方式运行效率有所降低，但可以用于没有专用负载均衡器且服务器数量有限的场景。\n使用专用负载均衡器。您可以使用云环境提供的负载均衡器作为各控制平面节点的反向代理。这种方式要求 KubeSphere 企业版集群安装在云环境中，并且云环境可以提供专用负载均衡器。\n使用通用服务器作为负载均衡器。您可以在集群节点以外的 Linux 服务器上安装 Keepalived 和 HAProxy 作为负载均衡器。这种方式需要至少 2 台额外的 Linux 服务器。\n使用本地负载均衡配置 如需使用 HAProxy 实现高可用性，只需要在安装 KubeSphere 企业版时在安装配置文件 config-sample.yaml 中设置以下参数：\nspec: controlPlaneEndpoint: internalLoadbalancer: haproxy domain: lb.kubesphere.local address: \u0026#34;\u0026#34; port: 6443 KubeKey 将自动在工作节点上安装 HAProxy 并完成高可用配置，您无需进行其他操作。有关更多信息请参阅安装 Kubernetes 和 KubeSphere 企业版。\n使用专用负载均衡器 如需使用云环境提供的专用负载均衡器实现高可用性，您需要在云环境中进行以下操作：\n在云环境中创建一台至少包含两个副本的负载均衡器。\n设置负载均衡器监听 KubeSphere 企业版集群各控制平面节点的 6443 端口。\n获取负载均衡器的 IP 地址，供后续安装 KubeSphere 企业版时使用。\n有关具体操作，请参阅云环境的用户指南或联系您的云服务提供商。\n使用通用服务器作为负载均衡器 以下介绍如何使用 Keepalived 和 HAProxy 将通用服务器配置成负载均衡器。\n前提条件 您需要准备 2 台与集群节点属于同一私有网络的 Linux 服务器，用作负载均衡器。\n您需要准备一个虚拟 IP 地址（VIP）用作 2 台负载均衡器服务器的浮动 IP 地址。该地址不应被其他设备或组件占用以免出现地址冲突。\n配置高可用性 登录用作负载均衡器的服务器，执行以下命令安装 HAProxy 和 Keepalived（以下以 Ubuntu 操作系统为例，在其他操作系统中请将 apt 替换为操作系统对应的软件包管理工具）：\napt install keepalived haproxy psmisc -y 执行以下命令编辑 HAProxy 的配置文件：\nvi /etc/haproxy/haproxy.cfg 在 HAProxy 的配置文件中添加以下信息并保存文件（将 \u0026lt;IP address\u0026gt; 替换为 KubeSphere 企业版集群各控制平面节点的私网 IP 地址）：\nglobal log /dev/log local0 warning chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy daemon stats socket /var/lib/haproxy/stats defaults log global option httplog option dontlognull timeout connect 5000 timeout client 50000 timeout server 50000 frontend kube-apiserver bind *:6443 mode tcp option tcplog default_backend kube-apiserver backend kube-apiserver mode tcp option tcplog option tcp-check balance roundrobin default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100 server kube-apiserver-1 \u0026lt;IP address\u0026gt;:6443 check server kube-apiserver-2 \u0026lt;IP address\u0026gt;:6443 check server kube-apiserver-3 \u0026lt;IP address\u0026gt;:6443 check 执行以下命令重启 HAProxy：\nsystemctl restart haproxy 执行以下命令设置 HAProxy 开机后自动运行：\nsystemctl enable haproxy 执行以下命令编辑 Keepalived 的配置文件：\nvi /etc/keepalived/keepalived.conf 在 Keepalived 的配置文件中添加以下信息并保存文件：\nglobal_defs { notification_email { } router_id LVS_DEVEL vrrp_skip_check_adv_addr vrrp_garp_interval 0 vrrp_gna_interval 0 } vrrp_script chk_haproxy { script \u0026#34;killall -0 haproxy\u0026#34; interval 2 weight 2 } vrrp_instance haproxy-vip { state BACKUP priority 100 interface \u0026lt;NIC\u0026gt; virtual_router_id 60 advert_int 1 authentication { auth_type PASS auth_pass 1111 } unicast_src_ip \u0026lt;source IP address\u0026gt; unicast_peer { \u0026lt;peer IP address\u0026gt; } virtual_ipaddress { \u0026lt;floating IP address\u0026gt; } track_script { chk_haproxy } } 将以下参数替换为实际值：\n参数 描述 \u0026lt;NIC\u0026gt;\n当前负载均衡器的网卡名称。\n\u0026lt;source IP address\u0026gt;\n当前负载均衡器的 IP 地址。\n\u0026lt;peer IP address\u0026gt;\n另一台负载均衡器的 IP 地址。\n\u0026lt;floating IP address\u0026gt;\n用作浮动 IP 地址的虚拟 IP 地址。\n执行以下命令重启 Keepalived：\nsystemctl restart keepalived 执行以下命令设置 Keepalived 开机后自动运行：\nsystemctl enable keepalived 重复以上步骤在另一台负载均衡器服务器上安装、配置 HAProxy 和 Keepalived。\n记录浮动 IP 地址，供后续安装 KubeSphere 企业版时使用。\n验证高可用性 登录第一台负载均衡器服务器并执行以下命令查看浮动 IP 地址：\nip a s 如果系统高可用性正常，命令回显中将显示已配置的浮动 IP 地址。例如，在以下命令回显中，inet 172.16.0.10/24 scope global secondary eth0 表明浮动 IP 地址已与 eth0 网卡绑定：\n1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eth0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 52:54:9e:27:38:c8 brd ff:ff:ff:ff:ff:ff inet 172.16.0.2/24 brd 172.16.0.255 scope global noprefixroute dynamic eth0 valid_lft 73334sec preferred_lft 73334sec inet 172.16.0.10/24 scope global secondary eth0 valid_lft forever preferred_lft forever inet6 fe80::510e:f96:98b2:af40/64 scope link noprefixroute valid_lft forever preferred_lft forever 执行以下命令模拟当前负载均衡器服务器故障：\nsystemctl stop haproxy 执行以下命令再次检查浮动 IP 地址：\nip a s 如果系统高可用性正常，命令回显中将不再显示浮动 IP 地址，如以下命令回显所示：\n1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eth0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 52:54:9e:27:38:c8 brd ff:ff:ff:ff:ff:ff inet 172.16.0.2/24 brd 172.16.0.255 scope global noprefixroute dynamic eth0 valid_lft 72802sec preferred_lft 72802sec inet6 fe80::510e:f96:98b2:af40/64 scope link noprefixroute valid_lft forever preferred_lft forever 登录另一台负载均衡器服务器，执行以下命令查看浮动 IP 地址：\nip a s 如果系统高可用性正常，命令回显中将显示已配置的浮动 IP 地址。例如，在以下命令回显中，inet 172.16.0.10/24 scope global secondary eth0 表明浮动 IP 地址已与 eth0 网卡绑定：\n1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eth0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 52:54:9e:3f:51:ba brd ff:ff:ff:ff:ff:ff inet 172.16.0.3/24 brd 172.16.0.255 scope global noprefixroute dynamic eth0 valid_lft 72690sec preferred_lft 72690sec inet 172.16.0.10/24 scope global secondary eth0 valid_lft forever preferred_lft forever inet6 fe80::f67c:bd4f:d6d5:1d9b/64 scope link noprefixroute valid_lft forever preferred_lft forever 在第一台负载均衡器服务器上执行以下命令恢复运行 HAProxy：\nsystemctl start haproxy ","href":"/v4.1.3/03-installation-and-upgrade/01-preparations/03-configure-high-availability/","isSection":null,"linkkey":null,"title":"配置高可用性"},{"content":" 本节仅介绍如何从 KubeSphere 企业版 v4.1.2 升级至 v4.1.3，不支持从 v3.5 直接升级至 v4.1.3。\n若要从 v3.5 升级至 v4.1.3，请联系 KubeSphere 企业版的交付服务专家获取升级文档，先升级至 v4.1.2。\n","href":"/v4.1.3/03-installation-and-upgrade/03-upgrade-kubesphere/","isSection":null,"linkkey":null,"title":"升级{ks_product_left}"},{"content":" NeonSAN 是青云的一款企业级分布式块存储系统，NeonSAN CSI 是 NeonSAN 团队面向 Kubernetes 提供的存储插件，具有在 Kubernetes 平台上动态创建持久存储卷的能力。\n本节介绍如何在生产环境中为 KubeSphere 企业版集群配置 NeonSAN CSI。\n前提条件 您已经成功部署 NeonSAN v2.2.0 及以上版本，且容器集群的每个节点都已安装 QBD 与 NeonSAN 连通。具体操作请咨询青云科技官方解决方案专家或交付服务专家。\n您已经安装 Kubernetes v1.16 及其以上版本。\n您已经在容器集群的 master 节点安装了 Helm。本节以 Helm 3 为例。\n操作步骤 在线安装 NeonSAN CSI 在线安装适用于容器集群能够访问外网的情况。\n执行以下命令添加 Helm 仓库，如 https://charts.kubesphere.io/test。\n$ helm repo add ks-test https://charts.kubesphere.io/test \u0026#34;ks-test\u0026#34; has been added to your repositories 执行以下命令查看仓库是否添加成功。\n$ helm repo list NAME URL ks-test https://charts.kubesphere.io/test 执行以下命令更新仓库的 Chart 列表。\n$ helm repo update 执行以下命令在仓库中查找 NeonSAN CSI 的安装包。\n$ helm search repo neonsan NAME CHART VERSION APP VERSION DESCRIPTION ks-test/csi-neonsan 1.2.2 1.2.0 A Helm chart for NeonSAN CSI Driver 查看 Master 节点上安装的 qbd 版本。\n$ qbd -v Package Version: 2.2.0-336092c-202202101432-ubuntu2004 Loaded Module Version: 2.2.0-336092c-202209010306-testlangchaor01n01 NeonSAN Static Library Version: 3.0.0-092498bf NeonSAN Protocol Version: 1 执行以下命令安装 NeonSAN CSI。根据 Master 节点操作系统上安装的 qbd 版本，设置参数 driver.repository，例如如果查询到的 qbd 版本为 2.2.0，则命令里的参数为 driver.repository=\u0026#34;csiplugin/csi-neonsan-qbd2.2.0\u0026#34;。\n$ helm install csi-neonsan ks-test/csi-neonsan --namespace kube-system --set driver.tag=\u0026#34;v1.2.3\u0026#34; --set sc.rep_count=2 --set driver.repository=\u0026#34;csiplugin/csi-neonsan-qbd2.2.0\u0026#34; NAME: csi-neonsan LAST DEPLOYED: Fri Nov 20 10:28:32 2020 NAMESPACE: kube-system STATUS: deployed REVISION: 1 TEST SUITE: None 执行以下命令检查 NeonSAN CSI 是否安装成功。\n$ helm list -n kube-system NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION csi-neonsan kube-system 1 2020-11-20 10:28:32.240990384 +0800 CST deployed csi-neonsan-1.2.2 1.2.0 检查 pod 是否在 Running 状态。\n$ kubectl get pod -n kube-system | grep csi-neonsan kube-system csi-neonsan-controller-75dc5cbcff-6gk54 5/5 Running 0 38s kube-system csi-neonsan-node-8vd8l 2/2 Running 0 38s kube-system csi-neonsan-node-dxk2z 2/2 Running 0 38s kube-system csi-neonsan-node-mp2b2 2/2 Running 0 38s 检查是否所有 NeonSAN CSI 组件运行正常。\n当 READY 取值等于 AVAILABLE 取值时，csi-neonsan-controller 正常。\n$ kubectl -n kube-system get deployments.apps csi-neonsan-controller NAME READY UP-TO-DATE AVAILABLE AGE csi-neonsan-controller 1/1 1 1 66m 当 DESIRED 取值等于 READY 和 AVAILABLE 取值时，csi-neonsan-node 正常。\n$ kubectl -n kube-system get daemonsets.apps csi-neonsan-node NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE csi-neonsan-node 3 3 3 3 3 \u0026lt;none\u0026gt; 66m 检查存储类是否安装。\n$ kubectl get storageclass NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE csi-neonsan neonsan.csi.qingstor.com Delete Immediate true 2m56s 查看存储类所使用的存储池。此存储池必须在 NeonSAN 中存在, 否则无法使用此存储类创建存储卷。\n$ kubectl get storageclass csi-neonsan -o yaml | grep pool_name pool_name: kube 登陆 NeonSAN 服务器, 查看存储类所使用的存储池是否存在，如不存在，请使用 neonsan create_pool 命令创建存储池。\n$ neonsan list_pool -pool kube -detail Pool Count: 1 +----------+------+---------------------------+ | ID | NAME | CREATED TIME | +----------+------+---------------------------+ | 33554432 | kube | 2020-08-07T14:53:52+08:00 | +----------+------+---------------------------+ 离线安装 NeonSAN CSI 离线安装适用于容器集群无法访问外网的情况。\n在本机上，下载 NeonSAN CSI 安装包，并安装包拷贝至集群 Master 节点。\n$ helm repo add ks-test https://charts.kubesphere.io/test \u0026#34;ks-test\u0026#34; has been added to your repositories $ helm pull ks-test/csi-neonsan $ ls -l csi-neonsan*.tgz -rw-r--r--. 1 root root 5196 Nov 20 13:13 csi-neonsan-1.2.2.tgz 执行以下命令查看 NeonSAN CSI 所需要的所有镜像文件。\n$ helm show values ks-test/csi-neonsan driver: repository: csiplugin/csi-neonsan tag: v1.2.0 node: repository: csiplugin/csi-neonsan-ubuntu tag: v1.2.0 provisioner: repository: csiplugin/csi-provisioner tag: v1.5.0 volumeNamePrefix: pvc attacher: repository: csiplugin/csi-attacher tag: v2.1.1 resizer: repository: csiplugin/csi-resizer tag: v0.4.0 snapshotter: repository: csiplugin/csi-snapshotter tag: v2.0.1 registrar: repository: csiplugin/csi-node-driver-registrar tag: v1.2.0 使用 Docker 将所有镜像下载到本地并打包，或上传至内网仓库（如 harbor）中。\ndocker pull csiplugin/csi-neonsan:v1.2.0 docker pull csiplugin/csi-neonsan-ubuntu:v1.2.0 docker pull csiplugin/csi-provisioner:v1.5.0 docker pull csiplugin/csi-attacher:v2.1.1 docker pull csiplugin/csi-resizer:v0.4.0 docker pull csiplugin/csi-snapshotter:v2.0.1 docker pull csiplugin/csi-node-driver-registrar:v1.2.0 docker save csiplugin/csi-neonsan:v1.2.0 \\ csiplugin/csi-neonsan-ubuntu:v1.2.0 \\ csiplugin/csi-provisioner:v1.5.0 \\ csiplugin/csi-attacher:v2.1.1 \\ csiplugin/csi-resizer:v0.4.0 \\ csiplugin/csi-snapshotter:v2.0.1 \\ csiplugin/csi-node-driver-registrar:v1.2.0 \\ -o neonsan-csi-images.tar 执行以下命令将镜像包上传至集群所有节点的目录，如 /tmp 目录下，解压并安装。\n$ scp neonsan-csi-images.tar user@node1:/tmp/ scp neonsan-csi-images.tar user@node2:/tmp/ ... $ tar -xvf /tmp/neonsan-csi-images.tar -C / 执行以下命令检查安装是否完成。如果您看到所有的 NeonSAN CSI 镜像已经在列表中，那么说明安装成功。\n$ docker images 参考在线安装中步骤 8 - 12 执行安装后检查。\nNeonSAN CSI 安装成功后，您可以在 KubeSphere 企业版控制台上的存储区域查看。\n","href":"/v4.1.3/03-installation-and-upgrade/01-preparations/04-configure-external-persistent-storage/04-configure-neosan-csi/","isSection":null,"linkkey":null,"title":"配置 NeonSAN CSI"},{"content":" 本节介绍如何在生产环境中为 KubeSphere 企业版集群配置外部持久化存储系统。持久化存储系统可用于创建卷供 KubeSphere 企业版平台上的应用存储数据。如未配置外部持久化存储系统，KubeSphere 企业版默认使用集群节点的本地存储系统。如果 KubeSphere 企业版集群不需要使用外部持久化存储，您可以跳过本节。\n说明 KubeSphere 企业版外部持久化存储配置仅支持同时安装 Kubernetes 和 KubeSphere 企业版的场景。如果您在现有的 Kubernetes 集群上安装 KubeSphere 企业版，KubeSphere 企业版安装完成后将使用 Kubernetes 集群现有的持久化存储配置。\n如何安装存储系统 安装 KubeSphere 企业版时，可以安装不同的存储系统作为插件。KubeKey 会为集群创建一个配置文件（默认为 config-sample.yaml），其中包含定义不同资源（包括存储插件）的全部必要参数。若要让 KubeKey 以预期的方式来安装这些存储系统，就必须为 KubeKey 提供这些存储系统的必要配置。\n通常，有两种方法能使 KubeKey 应用即将安装的存储系统的配置。\n直接在 config-sample.yaml 中的 addons 字段下输入必要的参数。\n为插件创建一个单独的配置文件，列出所有必要的参数，并在 config-sample.yaml 中提供文件的路径，以便 KubeKey 在安装过程中引用该路径。\n有关更多信息，请参见插件。\n默认存储类 KubeKey 支持安装不同的存储插件和存储类型。无论您要安装哪种存储系统，都可以在其配置文件中指定是否设为默认存储类。\n如果打算安装多个存储插件，那么只能将其中一个设置为默认存储类。否则，KubeKey 将无法识别使用哪种存储类型。\n","href":"/v4.1.3/03-installation-and-upgrade/01-preparations/04-configure-external-persistent-storage/","isSection":null,"linkkey":null,"title":"配置外部持久化存储"},{"content":" 本节介绍如何卸载 KubeSphere 企业版。\n","href":"/v4.1.3/03-installation-and-upgrade/04-uninstall-kubesphere/","isSection":null,"linkkey":null,"title":"卸载{ks_product_left}"},{"content":" 升级 Job 配置 选项 默认值 描述 upgrade.enabled\ntrue\nbool - 是否启用升级组件\nupgrade.image.registry\n\u0026#34;\u0026#34;\nstring - 设置升级 Job 的镜像仓库地址\nupgrade.image.repository\nkse/ks-upgrade\nstring - 设置升级 Job 的镜像名称\nupgrade.image.tag\n\u0026#34;\u0026#34;\nstring - 设置升级 Job 的镜像标签\nupgrade.image.pullPolicy\nAlways\nstring - 设置升级 Job 的镜像拉取策略\nupgrade.persistenceVolume.name\nks-upgrade\nstring - 设置升级 Job 的存储卷\nupgrade.persistenceVolume.storageClassName\n\u0026#34;\u0026#34;\nstring - 设置升级 Job 的存储卷类\nupgrade.persistenceVolume.accessMode\nReadWriteOnce\nstring - 设置升级 Job 的存储卷访问模式\nupgrade.persistenceVolume.size\n5Gi\nstring - 设置升级 Job 的存储卷容量\nupgrade.resources.limit.cpu\n1\nstring - 设置升级 Job 的 CPU 资源配额上限\nupgrade.resources.limit.memory\n1024Mi\nstring - 设置升级 Job 的 Memory 资源配额上限\nupgrade.resources.requests.cpu\n20m\nstring - 设置升级 Job 的 CPU 资源申请配额\nupgrade.resources.requests.memory\n100Mi\nstring - 设置升级 Job 的 Memory 资源申请配额\n升级应用配置 选项 默认值 描述 upgrade.config.storage.local.path\n/tmp/ks-upgrade\nstring - 设置升级 Job 使用本地目录存储备份数据\nupgrade.config.storage.s3.endpoint\n\u0026#34;\u0026#34;\nstring - 设置升级 Job 使用支持 S3 协议的对象存储服务备份数据，存储服务地址\nupgrade.config.storage.s3.region\n\u0026#34;\u0026#34;\nstring - 设置 S3 服务数据存储区域\nupgrade.config.storage.s3.disableSSL\nfalse\nbool - 设置禁用 SSL 安全协议\nupgrade.config.storage.s3.forcePathStyle\nfalse\nbool - 设置 S3 客户端对桶使用路径式寻址\nupgrade.config.storage.s3.accessKeyID\n\u0026#34;\u0026#34;\nstring - 设置 S3 服务账号的访问键，不使用凭证文件时是必需的\nupgrade.config.storage.s3.secretAccessKey\n\u0026#34;\u0026#34;\nstring - 设置 S3 服务账号的访问密钥，不使用凭证文件时是必需的\nupgrade.config.storage.s3.sessionToken\n\u0026#34;\u0026#34;\nstring - 设置 S3 服务的访问凭证文件\nupgrade.config.storage.s3.bucket\n\u0026#34;\u0026#34;\nstring - 设置 S3 服务的存储桶\nupgrade.config.download.globalRegistryUrl\noci://hub.kubesphere.com.cn/kse-extensions\nstring - 设置扩展组件的仓库地址\nupgrade.config.download.file\n-\n系统预留配置\nupgrade.config.download.http.timeout\n20\nint64 - 设置拉取扩展组件的超时时间\nupgrade.config.download.http.caBundle\n\u0026#34;\u0026#34;\nstring - 设置扩展组件仓库的自签名证书的 base64 字符串，多个自签名证书合并字符串 base64\nupgrade.config.download.http.insecureSkipVerify\ntrue\nbool - 设置跳过扩展组件仓库 TLS 认证\nupgrade.config.download.oci\n-\n系统预留配置\nupgrade.config.skipValidator\nfalse\nbool - 设置跳过升级 Job 版本校验，当前校验 KubeSphere 版本\n升级组件配置 选项 默认值 描述 upgrade.config.jobs.$ID.enabled\nfalse\nbool - 设置启用升级组件 $ID\nupgrade.config.jobs.$ID.priority\n0\nint - 设置组件升级顺序的优先级\nupgrade.config.jobs.$ID.extensionRef.name\n\u0026#34;\u0026#34;\nstring - 设置扩展组件名称\nupgrade.config.jobs.$ID.extensionRef.version\n\u0026#34;\u0026#34;\nstring - 设置扩展组件版本\n","href":"/v4.1.3/03-installation-and-upgrade/03-upgrade-kubesphere/05-appendix-ks-core/","isSection":null,"linkkey":null,"title":"附录 1：ks-core Helm Chart 升级参数"},{"content":" KubeSphere Helm Chart 选项 常用选项 选项 默认值 描述 adminPassword\n\u0026#34;\u0026#34;\nstring - 为第一个管理员用户设置引导密码。登录后，管理员需要重置密码。如不设置，会使用内置默认密码 P@88w0rd。\nhostname\n\u0026#34;example.com\u0026#34;\nstring - KubeSphere Server 完全限定的域名。\ningress.enabled\nfalse\nbool - 如果值为 true, 创建 KubeSphere Ingress 网关。\ningress.tls.enabled\ntrue\nbool - 如果值为 true, 则为 KubeSphere Ingress 网关启用 HTTPS。\ningress.tls.source\n\u0026#34;generation\u0026#34;\nstring - Ingress 证书的来源，可选项：\u0026#34;generation, importation, letsEncrypt\u0026#34;。\nletsEncrypt.email\n\u0026#34;\u0026#34;\nstring - 邮箱地址，证书过期时会收到邮件提醒。\nletsEncrypt.environment\n\u0026#34;production\u0026#34;\nstring - 可选项：\u0026#34;staging, production\u0026#34;。测试环境和生产环境证书在速率限制上存在区别，详情见：Let’s Encrypt 官方文档。\n高级选项 选项 默认值 描述 internalTLS\nfalse\nbool - 如果值为 true, 启用内部 TLS。console 和 apiserver 均会启用 HTTPS 服务。\ningress.ingressClassName\n\u0026#34;\u0026#34;\nstring - 使用网关的可选 Ingress 类，可选项：\u0026#34;nginx，traefik\u0026#34;。\ningress.secretName\n\u0026#34;kubesphere-tls-certs\u0026#34;\nstring - Ingress 网关所使用的包含 TLS 证书的 Secrets。\nextension.ingress.ingressClassName\n\u0026#34;\u0026#34;\n用于扩展组件访问的外部 ingress 的 ingressClassName。\nextension.ingress.domainSuffix\n\u0026#34;\u0026#34;\n用于创建扩展组件访问入口的域名后缀；根据外部 ingress 地址，它可以是 LB 主机名地址（比如 xx.com）、{node_ip}.nip.io 或内部 DNS 地址（比如 kse.local）。\nextension.ingress.httpPort\n80\n扩展组件的 ingress 的 http 端口。\nextension.ingress.httpsPort\n443\n扩展组件的 ingress 的 https 端口。\ncertmanager.duration\n2160h\nstring - cert-manager 生成证书的过期时间。\ncertmanager.renewBefore\n360h\nstring - cert-manager 刷新证书到证书过期之间的时间间隔。\nglobal.imageRegistry\nregistry.cn-beijing.aliyuncs.com\nstring - 设置全局的 KubeSphere 镜像仓库地址。\nglobal.tag\n\u0026#34;v4.1.3\u0026#34;\nstring - 设置全局的 KubeSphere 镜像仓库标签。\napiserver.image.registry\n\u0026#34;\u0026#34;\nstring - 设置 ks-apiserver 镜像仓库地址。\napiserver.image.repository\n\u0026#34;kse/ks-apiserver\u0026#34;\nstring - 设置 ks-apiserver 镜像名称。\napiserver.image.tag\n\u0026#34;\u0026#34;\nstring - 设置 ks-apiserver 镜像标签。\napiserver.nodePort\n\u0026#34;\u0026#34;\nuint16 - 设置 ks-apiserver 服务 service 的 NodePort 端口。\nconsole.image.registry\n\u0026#34;\u0026#34;\nstring - 设置 ks-console 镜像仓库地址。\nconsole.image.repository\n\u0026#34;kse/ks-console\u0026#34;\nstring - 设置 ks-console 镜像名称。\nconsole.image.tag\n\u0026#34;\u0026#34;\nstring - 设置 ks-console 镜像标签。\nconsole.nodePort\n30880\nuint16 - 设置 ks-console 服务 service 的 NodePort 端口。\ncontroller.image.registry\n\u0026#34;\u0026#34;\nstring - 设置 ks-controller-manager 镜像仓库地址。\ncontroller.image.repository\n\u0026#34;kse/ks-controller-manager\u0026#34;\nstring - 设置 ks-controller-manager 镜像名称。\ncontroller.image.tag\n\u0026#34;\u0026#34;\nstring - 设置 ks-controller-manager 镜像标签。\ncomposedApp.appSelector\n\u0026#34;\u0026#34;\nstring - 指定 annotation 或 label，以便对匹配的自制应用进行处理并更新状态。\n若 appSelector 值为空，则处理所有自制应用。若在命令或配置字典中添加以下内容，可指定只处理 KubeSphere 创建、管理的自制应用。\n安装 ks-core 时，在安装命令中添加：\n--set composedApp.appSelector=\u0026#34;kubesphere.io/creator\u0026#34;\n升级 ks-core 时，在升级命令中添加：\n--set composedApp.appSelector=\u0026#34;kubesphere.io/creator\u0026#34;\n安装升级完成后，在集群的 kubesphere-config 配置字典中设置为：\ncomposedApp: appSelector: \u0026#34;kubesphere.io/creator\u0026#34; TLS 配置 选择 SSL 配置\nKubeSphere 安全配置分为网关 SSL 配置以及内部服务 SSL 配置两个部分。其中网关 SSL 配置默认支持以下三种模式来启用 SSL/TLS，以保证访问的安全性。\n网关 SSL 配置\n配置 Helm Chart 选项 是否需要 cert-manager KubeSphere 生成的 TLS 证书\ningress.tls.source=generation\n否\nLet’s Encrypt\ningress.tls.source=letsEncrypt\n是\n导入已有的证书\ningress.tls.source=importation\n否\nKubeSphere 生成的 TLS 证书：支持 cert-manager 和 helm 两种方式。\n如果 Kubernetes 集群中已安装 cert-manager，则首选使用 cert-manager 生成证书。KubeSphere 使用 cert-manager 签发并维护证书。KubeSphere 会生成自己的 CA 证书，并用该 CA 签署证书，然后由 cert-manager 管理该证书。\n如果未安装 cert-manager，则使用 helm 生成证书。在使用 helm 安装的过程中，KubeSphere 会根据设置的 hostname 生成 CA 和 TLS 证书。在此选项下，证书不支持自动过期轮转。\nLet’s Encrypt\n使用 Let’s Encrypt 选项必须使用 cert-manager。但是，在这种情况下，cert-manager 与 Let’s Encrypt 的特殊颁发者相结合，该颁发者执行获取 Let’s Encrypt 颁发证书所需的所有操作，包括请求和验证。此配置使用 HTTP 验证（HTTP-01），因此负载均衡器必须具有可以从互联网访问的公共 DNS 记录。\n导入已有的证书\n使用已有的 CA 颁发的公有或私有证书。KubeSphere 将使用该证书来保护 WebSocket 和 HTTPS 流量。在这种情况下，您必须上传名称分别为 tls.crt 和 tls.key 的 PEM 格式的证书以及相关的密钥。如果您使用私有 CA，则还必须上传该 CA 证书。这是由于您的节点可能不信任此私有 CA。\n内部服务 SSL 配置\n启用内部 SSL 配置之后，KubeSphere 中 Console UI 和 Apiserver 均会启用 HTTPS，内置支持 cert-manager 和 helm 生成证书。在 Kubernetes 集群已安装 cert-manager 的情况下优先使用 cert-manager 生成/管理证书，证书的 DNS 默认使用 Console UI 和 Apiserver 在 Kubernetes 集群内部的 Service DNS。\n配置 Helm Chart 选项 是否需要 cert-manager 启用内部 SSL\ninternalTLS=true\n否\n安装 cert-manager\n若使用自己的证书文件（ingress.tls.source=importation），您可以跳过此步骤。\n仅在使用 KubeSphere 生成的证书（ingress.tls.source=generation）或 Let’s Encrypt 颁发的证书（ingress.tls.source=letsEncrypt）时，才需要安装 cert-manager。\n# 添加 Jetstack Helm 仓库 helm repo add jetstack https://charts.jetstack.io # 更新本地 Helm Chart 仓库缓存 helm repo update # 安装 cert-manager Helm Chart helm install cert-manager jetstack/cert-manager -n cert-manager --create-namespace --set prometheus.enabled=false # 或 kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/\u0026lt;VERSION\u0026gt;/cert-manager.yaml 安装完 cert-manager 后，检查 cert-manager 命名空间中正在运行的 Pod 来验证它是否已正确部署：\nkubectl get pods --namespace cert-manager 根据您选择的证书选项，通过 Helm 为 KubeSphere 开启 SSL 配置\n启用网关 SSL 配置\nKubeSphere 生成的证书\nhelm upgrade --install -n kubesphere-system --create-namespace ks-core $chart --version $version \\ --set ingress.enabled=true \\ --set hostname=kubesphere.my.org Let’s Encrypt\n此选项使用 cert-manager 来自动请求和续订 Let’s Encrypt 证书。Let’s Encrypt 是免费的，而且是受信的 CA，因此可以提供有效的证书。\nhelm upgrade --install -n kubesphere-system --create-namespace ks-core $chart --version $version \\ --set hostname=kubesphere.my.org \\ --set ingress.enabled=true \\ --set ingress.tls.source=letsEncrypt \\ --set letsEncrypt.email=me@example.org 导入外部证书\n# 导入外部证书 kubectl create secret tls tls-ks-core-ingress --cert=tls.crt --key=tls.key -n kubesphere-system # 安装 KubeSphere helm upgrade --install -n kubesphere-system --create-namespace ks-core $chart --version $version \\ --set ingress.enabled=true \\ --set hostname=kubesphere.my.org \\ --set ingress.tls.source=importation 启用内部服务 SSL 配置\nhelm upgrade --install -n kubesphere-system --create-namespace ks-core $chart --version $version \\ --set internalTLS=true 配置 ratelimit 限流器 启用限流器之后，限流器会对所有用户的请求独立限流，主要支持以下两种方式：\n对 KubeSphere 中的每个用户设置限流速率，暂不支持独立设置每个用户的限流速率；\n对 KubeSphere 中的每个 ServiceAccount 独立设置限流速率。\n启用限流器 启用限流器就是对 KubeSphere 中的每个用户设置限流速率。\n修改 kubesphere-system 配置文件。\nkubectl -n kubesphere-system edit cm kubesphere-system 新增以下内容：\nrateLimit: enable: true # 启用限流器 driver: memory # 内存模式 QPS: 40.0 # 令牌恢复速率 burst: 80 # 令牌桶容量 重启 ks-apiserver。\nkubectl -n kubesphere-system rollout restart deploy ks-apiserver 设置 ServiceAccount 限流器 设置前，您需要按照上一步启用限流器。然后执行以下命令对 ServiceAccount 设置限流速率。\nkubectl -n \u0026lt;Namespace\u0026gt; patch serviceaccounts.kubesphere.io \u0026lt;ServiceAccount\u0026gt; --type merge -p \u0026#39;{\u0026#34;metadata\u0026#34;: {\u0026#34;annotations\u0026#34;: {\u0026#34;kubesphere.io/ratelimiter-qps\u0026#34;: \u0026#34;20.0\u0026#34;, \u0026#34;kubesphere.io/ratelimiter-burst\u0026#34;: \u0026#34;40\u0026#34;}}}\u0026#39; 参数说明 选项 默认值 描述 rateLimit.enable\nfalse\nbool - 启用限流器。\nrateLimit.driver\nmemory\nstring - 限流器存储类型，可选项：\u0026#34;memory\u0026#34;。\nrateLimit.QPS\n5.0\nfloat32 - 限流器令牌桶算法中每秒恢复的令牌数。\nrateLimit.burst\n10\nint - 限流器令牌桶算法中令牌桶的最大容量。\n说明 令牌的恢复速率 QPS 建议设置为桶容量 burst 的一半。\n","href":"/v4.1.3/03-installation-and-upgrade/02-install-kubesphere/05-appendix/","isSection":null,"linkkey":null,"title":"附录：KubeSphere Core 高级配置"},{"content":" 本节介绍如何在生产环境中为 KubeSphere 企业版集群配置网络文件系统（NFS）。\n说明 NFS 与部分应用不兼容（例如 Prometheus），可能会导致容器组创建失败。如果确实需要在生产环境中使用 NFS，请确保您了解相关风险或咨询青云科技官方解决方案专家或交付服务专家。\n搭建 NFS 服务端 为 KubeSphere 企业版配置 NFS 前，您需要先搭建 NFS 服务端。如果您已经有可用的 NFS 服务端，您可以跳过此步骤。以下以 Ubuntu 操作系统安装操作 NFS Kernel Server 为例介绍如何搭建 NFS 服务端。有关其他 NFS 服务端和其他操作系统的具体操作，请参阅 NFS 服务端和操作系统的用户指南。\n前提条件 您需要准备一台 Linux 服务器（以下以 Ubuntu 操作系统为例），该服务器必须与 KubeSphere 企业版集群节点网络连通。\n操作步骤 登录用于搭建 NFS 服务端的服务器，执行以下命令指安装 NFS Kernel Server：\nsudo apt update sudo apt install nfs-kernel-server 执行以下命令创建供 KubeSphere 企业版使用的目录（将 \u0026lt;directory\u0026gt; 替换为实际的目录路径，例如 /mnt/demo）：\nsudo mkdir -p \u0026lt;directory\u0026gt; 执行以下命令移除目录的访问限制（将 \u0026lt;directory\u0026gt; 替换为实际的目录路径，例如 /mnt/demo）：\nsudo chown nobody:nogroup \u0026lt;directory\u0026gt; sudo chmod 777 \u0026lt;directory\u0026gt; 执行以下命令编辑 NFS Kernel Server 的配置文件：\nsudo vi /etc/exports 将 KubeSphere 企业版集群节点的信息添加到文件中从而允许服务器访问 NFS 服务端，并保存文件：\n\u0026lt;directory\u0026gt; \u0026lt;IP address\u0026gt;(rw,sync,no_subtree_check) 将以下参数替换为实际值：\n参数 描述 \u0026lt;directory\u0026gt;\n供 KubeSphere 企业版使用的目录，例如 /mnt/demo。\n\u0026lt;IP address\u0026gt;\nKubeSphere 企业版集群节点的 IP 地址，例如 192.168.0.2。\n如果存在多个 KubeSphere 企业版集群节点，请设置多个配置条目。您也可以将 \u0026lt;IP address\u0026gt; 设置为一个网段从而使该网段的所有服务器都能访问 NFS 服务端，例如 192.168.0.0/24。\n执行以下命令启用目录共享：\nsudo exportfs -a 执行以下命令重启 NFS 服务端使配置生效：\nsudo systemctl restart nfs-kernel-server 配置 KubeSphere 企业版集群节点 NFS 服务端搭建完成后，您需要在 KubeSphere 企业版集群节点上安装客户端工具，并创建配置文件供后续安装 KubeSphere 企业版时使用。KubeSphere 企业版安装完成后将使用配置文件指定的 NFS 服务端作为持久化存储。以下以 Ubuntu 操作系统安装 NFS Common 为例介绍如何安装 NFS 客户端以及创建配置文件。有关其他 NFS 客户端和其他操作系统的具体操作，请参阅 NFS 客户端和操作系统的用户指南。\n前提条件 您需要搭建 NFS 服务端。有关更多信息，请参阅搭建 NFS 服务端。\n操作步骤 登录所有 KubeSphere 企业版集群节点，执行以下命令安装 nfs-common：\nsudo apt update sudo apt install nfs-common 登录用于执行 KubeSphere 企业版安装操作的集群节点，执行以下命令创建 NFS 存储插件配置文件：\nvi nfs-client.yaml 将以下信息添加到配置文件中，并保存文件供后续安装 KubeSphere 企业版时使用：\nnfs: server: \u0026#34;\u0026lt;IP address\u0026gt;\u0026#34; path: \u0026#34;\u0026lt;directory\u0026gt;\u0026#34; storageClass: defaultClass: true 将以下参数替换为实际值：\n参数 描述 \u0026lt;IP address\u0026gt;\nNFS 服务端的 IP 地址。\n\u0026lt;directory\u0026gt;\nNFS 服务端供 KubeSphere 企业版使用的目录。\n以上配置文件仅包含必须设置的参数。如需设置其他参数，请参阅 NFS Client 配置。\n","href":"/v4.1.3/03-installation-and-upgrade/01-preparations/04-configure-external-persistent-storage/05-configure-nfs/","isSection":null,"linkkey":null,"title":"配置 NFS"},{"content":" 本节介绍如何添加和删除 KubeSphere 企业版集群节点。\n","href":"/v4.1.3/03-installation-and-upgrade/05-add-and-delete-cluster-nodes/","isSection":null,"linkkey":null,"title":"添加和删除集群节点"},{"content":" KubeSphere 支持众多第三方开源存储系统，包括但不限于：\nCeph CSI\nGlusterFS\nOpenEBS\nLonghorn\nKubeSphere 企业版不提供第三方开源存储系统的安装包，请到存储系统的官方网站下载安装。\n","href":"/v4.1.3/03-installation-and-upgrade/01-preparations/04-configure-external-persistent-storage/07-configure-opensource-storage/","isSection":null,"linkkey":null,"title":"配置开源存储系统"},{"content":"","href":"/v4.1.3/","isSection":[{"children":[{"children":[{"title":"产品优势","url":"01-intro/01-introduction/"},{"title":"应用场景","url":"01-intro/03-scenarios/"}],"title":"产品简介"}],"icon":"icon-note_tree_duotone","title":"了解"},{"children":[{"children":[{"title":"安装示例组件","url":"02-quickstart/02-install-an-extension"},{"title":"控制用户权限","url":"02-quickstart/03-control-user-permissions"},{"title":"部署示例应用","url":"02-quickstart/06-deploy-bookinfo"}],"title":"快速入门"}],"icon":"icon-gamepad_2_duotone","title":"上手"},{"children":[{"children":[{"title":"扩展组件管理","url":"06-extension-user-guide"},{"title":"集群管理","url":"07-cluster-management"},{"title":"企业空间管理","url":"08-workspace-management"},{"title":"项目管理","url":"09-project-management"}],"title":"管理指南"}],"icon":"icon-simulation_duotone","title":"管理"},{"children":[{"children":[{"title":"DevOps","url":"11-use-extensions/01-devops"},{"title":"应用商店管理","url":"11-use-extensions/02-app-store"},{"title":"Whizard 可观测平台","url":"11-use-extensions/05-observability-platform"},{"title":"KubeSphere 网络","url":"11-use-extensions/11-network"}],"title":"扩展组件"}],"icon":"icon-apps_2_duotone","title":"使用"}],"linkkey":null,"title":"KubeSphere 企业版"},{"content":"","firstChild":{"href":"/categories/","title":"Categories"},"href":"/categories/","isSection":null,"linkkey":null,"title":"KubeSphere 企业版"},{"content":"","firstChild":{"href":"/v4.1.3/03-installation-and-upgrade/01-preparations/01-supported-k8s/","title":"环境要求"},"href":"/","isSection":null,"linkkey":null,"title":"KubeSphere 企业版"},{"content":"","firstChild":{"href":"/tags/","title":"Tags"},"href":"/tags/","isSection":null,"linkkey":null,"title":"KubeSphere 企业版"},{"content":"","firstChild":{"href":"/v4.1.3/search/","title":"搜索功能"},"href":"/v4.1.3/search/","isSection":null,"linkkey":null,"title":"搜索功能"}]